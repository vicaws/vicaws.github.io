<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://vicaws.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vicaws.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-13T10:02:34+00:00</updated><id>https://vicaws.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">CME E-mini S&amp;amp;P 500 options product summary</title><link href="https://vicaws.github.io/blog/2023/cme-sp500/" rel="alternate" type="text/html" title="CME E-mini S&amp;amp;P 500 options product summary"/><published>2023-02-26T15:38:00+00:00</published><updated>2023-02-26T15:38:00+00:00</updated><id>https://vicaws.github.io/blog/2023/cme-sp500</id><content type="html" xml:base="https://vicaws.github.io/blog/2023/cme-sp500/"><![CDATA[<h2 id="about-cmes-e-mini-sp-500">About CME’s E-mini S&amp;P 500</h2> <p>An electronically traded futures contract one fifth the size of standard S&amp;P futures, CME-listed <a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.html">E-mini S&amp;P 500 futures and options</a> are based on the underlying Standard &amp; Poor’s 500 stock index. Made up of 500 individual stocks representing the market capitalizations of large companies, the S&amp;P 500 Index is a leading indicator of large-cap U.S. equities.</p> <p>While the futures contracts have relatively straightforward <a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.html">specs</a>, CME has much more complex offerings of options that are settled into those futures. This allows traders and investors to have OTC-like flexibilities while also taking advantage of the liquidity of a central, electronic exchange market. In this post, we summarize the various option offerings and distinguish them by a few key features.</p> <h2 id="option-product-summary">Option product summary</h2> <table> <thead> <tr> <th><strong>Name</strong></th> <th><strong>Globex Code</strong></th> <th><strong>Style</strong></th> <th><strong>Listing Rules</strong></th> <th><strong>Last Trading Date</strong></th> </tr> </thead> <tbody> <tr> <td><a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html#optionProductId=138">E-mini S&amp;P 500 Options</a></td> <td>ES</td> <td>American</td> <td>Quarterly contracts (Mar, Jun, Sep, Dec) listed for 9 consecutive quarters and 3 additional December contract months</td> <td>Trade terminates on the 3rd Friday of the contract quarter</td> </tr> <tr> <td><a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html#optionProductId=10032">E-mini S&amp;P 500 Quarterly PM Options</a></td> <td>EY</td> <td>European</td> <td>Quarterly contracts (Mar, Jun, Sep, Dec) listed for 4 consecutive quarters</td> <td>Trade terminates on the last business day of the contract month</td> </tr> <tr> <td><a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html#optionProductId=8292">E-mini S&amp;P 500 Monday Weekly Options</a> - Week1/2/3/4/5</td> <td>E1A, E2A, E3A, E4A, E5A</td> <td>European</td> <td>Weekly contracts listed for 5 consecutive weeks<d-footnote>No weekly contract listed if the expiration would occur on the same day as the monthly option expiration.</d-footnote></td> <td>Trade terminates on Monday of the contract month</td> </tr> <tr> <td><a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html#optionProductId=10132">E-mini S&amp;P 500 Thuesday Weekly Options</a> - Week1/2/3/4/5</td> <td>E1B, E2B, E3B, E4B, E5B</td> <td>European</td> <td>Weekly contracts listed for 5 consecutive weeks<d-footnote>No weekly contract listed if the expiration would occur on the same day as the monthly option expiration.</d-footnote></td> <td>Trade terminates on Tuesday of the contract month</td> </tr> <tr> <td><a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html#optionProductId=8227">E-mini S&amp;P 500 Wednesday Weekly Options</a> - Week1/2/3/4/5</td> <td>E1C, E2C, E3C, E4C, E5C</td> <td>European</td> <td>Weekly contracts listed for 5 consecutive weeks<d-footnote>No weekly contract listed if the expiration would occur on the same day as the monthly option expiration.</d-footnote></td> <td>Trade terminates on Wednesday of the contract month</td> </tr> <tr> <td><a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html#optionProductId=10137">E-mini S&amp;P 500 Thursday Weekly Options</a> - Week1/2/3/4/5</td> <td>E1D, E2D, E3D, E4D, E5D</td> <td>European</td> <td>Weekly contracts listed for 5 consecutive weeks<d-footnote>No weekly contract listed if the expiration would occur on the same day as the monthly option expiration.</d-footnote></td> <td>Trade terminates on Thursday of the contract month</td> </tr> <tr> <td><a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html#optionProductId=136">E-mini S&amp;P 500 EOM Options</a></td> <td>EW</td> <td>European</td> <td>Monthly contracts listed for 6 consecutive months, plus 4 quarterly months</td> <td>Trade terminates on the last business day of the contract month</td> </tr> <tr> <td><a href="https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.contractSpecs.options.html#optionProductId=2915">E-mini S&amp;P 500 Weekly Options</a></td> <td>EW1, EW2, EW3, EW4</td> <td>European</td> <td>6 weekly contracts listed for week 1/2/4 and 9 weekly contracts listed for week 3</td> <td>Trade terminates on Friday of the contract week</td> </tr> </tbody> </table> <p><strong>Recommended tools:</strong></p> <ul> <li> <p><a href="https://www.cmegroup.com/tools-information/quikstrike/options-calendar-equity-index.html">Option expiration calendar</a></p> </li> <li> <p><a href="https://www.cmegroup.com/reports/equities-market-data-overview.pdf">Equity index options daily trading recap</a></p> </li> <li> <p><a href="https://www.cmegroup.com/reports/daily-index-option-spread-activity.pdf">Equity index options daily spread trading recap</a></p> </li> </ul> <h2 id="liquidity">Liquidity</h2> <p>Though the above option offerings seem to have quite an exhaustive coverage of time-to-expirations, unsurprisingly only few of them have significant trading volumes. In the following screenshot, we show the top traded equity index options and futures as of 24th February 2023. Check <a href="https://www.cmegroup.com/trading/equity-index/options-on-futures.html">here</a> for up-to-date figures.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/cme-sp500/liquidity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name>Victor Wang</name></author><category term="finance"/><category term="finance"/><summary type="html"><![CDATA[About CME’s E-mini S&amp;P 500]]></summary></entry><entry><title type="html">Sparse representation of signals with the K-SVD algorithm</title><link href="https://vicaws.github.io/blog/2018/ksvd/" rel="alternate" type="text/html" title="Sparse representation of signals with the K-SVD algorithm"/><published>2018-02-26T12:00:00+00:00</published><updated>2018-02-26T12:00:00+00:00</updated><id>https://vicaws.github.io/blog/2018/ksvd</id><content type="html" xml:base="https://vicaws.github.io/blog/2018/ksvd/"><![CDATA[<p>We provide an executive summary of the paper by Michal Aharon, Michael Elad and Alfred Bruckstein <d-cite key="aharon"></d-cite> that proposes the novel K-SVD algorithm for designing overcomplete dictionaries for signals sparse representation. In the next sections, we firstly describe the mathematics of signals sparse representation, and then explain how the K-SVD algorithm works in solving the problem. Finally, we present some synthetic experiments to test our implementation of the K-SVD algorithm.</p> <h2 id="sparse-representations-of-signals">Sparse representations of signals</h2> <p>Suppose a signal is denoted by a numeric vector \(\mathbf{y} \in \mathbb{R}^n\), the sparse representation of the signal is \(\mathbf{y} \approx \mathbf{D} \mathbf{x}\), where \(\mathbf{D} \in \mathbb{R}^{n \times K}\) is called the overcomplete dictionary matrix assuming \(n &lt; K\) and \(\mathbf{x} \in \mathbb{R}^K\) is a sparse vector. In other words, the goal is to represent \(\mathbf{y}\) as a sparse linear combination of the \(K\) prototype signal atoms \(\{\mathbf{d}_j\}^K_{j=1}\) that form columns of the dictionary \(\mathbf{D}\).</p> <p>The application of sparse representation is to train a proper dictionary \(\mathbf{D}\) based on a set of signals \(\mathbf{Y} = \{\mathbf{y}_j\}_{j=1}^N\), so that all newly coming signals can be possibly stored at minimum space as a sparse vector by taking advantage of this dictionary. This can be formulated as,</p> \[\begin{equation} \min_{\mathbf{D}, \mathbf{X}} \left\{ ||\mathbf{Y} - \mathbf{DX}||_F^2 \right\}, \text{ s.t. } \forall i, ||\mathbf{x}_i||_0 \leq T_0, \label{eq:formula} \end{equation}\] <p>where \(\mathbf{X} = \{\mathbf{x}_i\}_{i=1}^{N} \in \mathbb{R}^{K \times N}\), and \(T_0\) defines the sparsity requirement. The objective targets to minimise the Frobenius norm by optimising the choice of dictionary \(\mathbf{D}\) and sparse matrix \(\mathbf{X}\).</p> <p>The algorithms of solving the sparse representation problem (\ref{eq:formula}) typically involves iteratively solving \(\mathbf{X}\) and \(\mathbf{D}\) in sequence, given the other variable as a known parameter. In fact, literatures denote the corresponding two subproblems,</p> <ul> <li> <p><strong>Sparse coding</strong> is to compute the representation coefficients \(\mathbf{x}\) based on the given signal \(\mathbf{y}\) and the dictionary \(\mathbf{D}\) by solving \eqref{eq:formula}.</p> </li> <li> <p><strong>Dictionary update</strong> is to search for better dictionary \(\mathbf{D}\) based on the given signals \(\mathbf{Y}\) and the solved coefficients matrix \(\mathbf{X}\) in the previous sparse coding stage. By “better” we mean to further minimise the objective in \eqref{eq:formula}.</p> </li> </ul> <h2 id="the-k-svd-algorithm">The K-SVD algorithm</h2> <p>The K-SVD algorithm introduced by Michal Aharon, Michael Elad and Alfred Bruckstein <d-cite key="aharon"></d-cite> focuses on solving the dictionary update problem as explained in the previous section. The K-SVD algorithm is a generalisation of the K-means clustering algorithm that updates dictionary columns along with a simultaneous update of the associated sparse representations, thus accelerating the convergence.</p> <p>To see this, we assume that both \(\mathbf{X}\) and \(\mathbf{D}\) are fixed, and we would like to update only one column of dictionary \(\mathbf{d}_k\), and the associated sparse representations \(\mathbf{x}^k_T\), namely the \(k\)-th row in \(\mathbf{X}\). Then the objective in (\ref{eq:formula}) is rewritten as,</p> \[\begin{align} ||\mathbf{Y} - \mathbf{D} \mathbf{X}||_F^2 &amp; = \left\Vert \left( \mathbf{Y} - \sum_{j \neq k} \mathbf{d}_j \mathbf{x}^k_T \right) - \mathbf{d}_k \mathbf{x}_T^k \right\Vert_F^2 \\ &amp; := \left\Vert \mathbf{E}_k - \mathbf{d}_k \mathbf{x}_T^k \right\Vert_F^2. \label{eq:errorUpdateD} \end{align}\] <p>The matrix \(\mathbf{E}_k\) represents the error for all the \(N\) samples when the \(k\)-th atom is removed. We can use SVD to find the closest rank-1 matrix that approximates \(\mathbf{E}_k\), which effectively minimizes (\ref{eq:errorUpdateD}). However, the resulting \(\mathbf{x}_T^k\) is unlikely to fulfil the sparsity requirements. The remedy is to require that the new \(\tilde{\mathbf{x}}_T^k\) to have the same support as the original \(\mathbf{x}_T^k\). To add the requirement, we revise the objective (\ref{eq:errorUpdateD}) to</p> <p>\begin{equation} \left\Vert \mathbf{E}_k \mathbf{\Omega}_k - \mathbf{d}_k \mathbf{x}_T^k \mathbf{\Omega}_k \right\Vert_F^2, \end{equation} where</p> \[\begin{equation} \mathbf{\Omega}_k = \begin{cases} 1 \text{, } i = \omega_k(j); \\ 0 \text{, otherwise;} \end{cases} \end{equation}\] <p>and that the support of \(\mathbf{x}_T^k\) is defined as \(\omega_k := \{i: 1 \leq i \leq K, \text{ } \mathbf{x}_T^k \neq 0 \}\). Multiplying \(\mathbf{\Omega}_k\) shrinks \(\mathbf{x}_T^k\) by discarding zero entries and also refines \(\mathbf{E}_k\) to a selection of error columns that correspond to samples that use the atom \(\mathbf{d}_k\).</p> <p>Thereafter, we can decompose \(\mathbf{E}_k \mathbf{\Omega}_k = \mathbf{U} \Delta \mathbf{V}^\top\), and update:</p> <ul> <li> <p>\(\mathbf{d}_k\) as the first column of \(\mathbf{U}\);</p> </li> <li> <p>the non-zero elements of \(\mathbf{x}_T^k\) as the first column of \(\mathbf{V}\) multiplied by the largest singular value, namely \(\Delta (1,1)\)</p> </li> </ul> <p>By far, the K-SVD algorithm completes updating the dictionary, which will be used in the next iteration for sparse coding, unless the convergence requirement is satisfied (or maximum iterations is reached).</p> <h2 id="synthetic-experiments">Synthetic experiments</h2> <p>We’ve implemented the K-SVD algorithm in Python (see the <a href="#appendix">Appendix</a>). Meanwhile, we solve the sparse coding problem by taking advantage of the <code class="language-plaintext highlighter-rouge">OrthogonalMatchingPursuit()</code> implementation maintained by <code class="language-plaintext highlighter-rouge">Scikit-learn</code>. Together we are able to iteratively solve the sparse representation problem.</p> <p>To test the algorithm and our implementation, we follow the same way as described in \cite{aharon} to simulate synthetic data (section V.A.) and measure performance (equation (25) in section V.D.). Moreover, we define a more general performance measure</p> \[\begin{equation} P(\epsilon) = \sum_{k=1}^K \mathbf{1}_{\{1-|\mathbf{d}_i^\top \tilde{\mathbf{d}}_i|&lt; \epsilon \} }, \label{eq:performance} \end{equation}\] <p>which counts the number of “successes” in recovering the dictionary. Here \(\mathbf{d}_i\) is the generating dictionary atom and \(\tilde{\mathbf{d}}_i\) is its corresponding element (closest column in \(\ell^2\) norm) in the recovered dictionary.</p> <p>We need to initialise a dictionary to feed into the sparse coding process. Therefore it is reasonable to investigate the sensitivity of the result against the dictionary initialisation. Nevertheless, we show the result from one single trial first. In Figure 1, we plot the relative error (\(\|\mathbf{Y}-\mathbf{DX}\|_F / \|\mathbf{Y}\|_F\)) over iterations on the left, and the histogram of column \(\ell^2\)-norm errors on the right. The errors tend to converge into a positive constant. More than that, dictionary update by K-SVD algorithm greatly reduces the error during each iteration. Unfortunately, the orthogonal matching pursuit algorithm in the next iteration always brings the error back to a high level. We are thinking that this could be caused by that the function implemented by <code class="language-plaintext highlighter-rouge">Skicit-learn</code> does not allow us to initialize \(\mathbf{X}\) from previous iteration, so that it always leads to another worse local minimum. Looking at the histogram, we observe that 95% of the recovered signals (columns) have less than 20% errors from the original signals.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/ksvd/SingleSim.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 1</strong> - Error.</p> <p>Next we look at the performance measure as defined in (\ref{eq:performance}). In Figure 2, we show how the number of successes vary with tolerance. With \(\epsilon \geq 0.06\), we can obtain at least 45 successes out of 50 for dictionary recovery.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/ksvd/SingleSimSuccess.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 2</strong> - Count of successes.</p> <p>As mentioned before, it is reasonable to investigate the sensitivity of the result against the dictionary initialization. We initialize the dictionary with i.i.d. uniformly distributed entries. Each column is then normalized to a unit \(\ell^2\)-norm. We show in Figure 3 how error evolves over iterations for 50 different initialisation. It turns out the errors are reasonably consistent by the end of iteration.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/ksvd/MultiSim.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 3</strong> - Errors of multiple trials.</p> <h2 id="appendix">Appendix</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">scipy</span> <span class="k">as</span> <span class="n">sp</span>
<span class="kn">import</span> <span class="n">scipy.sparse</span> <span class="k">as</span> <span class="n">sps</span>

<span class="kn">from</span> <span class="n">scipy.linalg</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">svd</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">rankdata</span>

<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">OrthogonalMatchingPursuit</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">gen_rand_dict_mat</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">):</span>
  <span class="sh">'''</span><span class="s">
  Simulate dictionary D with unit-L2-norm column.

  Arguments:

      n_features: int
          Number of features for signals

      n_atoms: int
          Number of signal atoms in the dictionary D.

  Returns:

      D: 2-D matrix, shape=(n_features, n_atoms)
          Simulated dictionary
  </span><span class="sh">'''</span>

  <span class="n">D_raw</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">)</span>
  <span class="n">col_norms</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">D_raw</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">D</span> <span class="o">=</span> <span class="n">D_raw</span> <span class="o">/</span> <span class="n">col_norms</span>

  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mat</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gen_rand_measure</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_sparsity</span><span class="p">):</span>
  <span class="sh">'''</span><span class="s">
  Simulate sparse representation coefficient matrices X. The number of non-zero
  elements equals to n_sparsity, their values are uniformly distributed in
  interval [0.1], and their locations are also uniformly random.

  Arguments:

      n_samples: int
          Number of samples

      n_atoms: int
          Number of signal atoms in the dictionary D.

      n_sparsity: int
          Maximal number of non-zero elements allowed in each measurment vector.

  Returns:

      X: 2-D sparse matrix, shape=(n_atoms, n_samples)
          Each column contains the coefficients for sparse representation of signals.   
  </span><span class="sh">'''</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">([</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">])</span>

  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
      <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">)</span>
      <span class="n">rank</span> <span class="o">=</span> <span class="nf">rankdata</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
      <span class="n">loc_indices</span> <span class="o">=</span> <span class="n">rank</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="n">n_sparsity</span><span class="p">]</span>

      <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">n_sparsity</span><span class="p">)</span>

      <span class="nf">for </span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">loc_indices</span><span class="p">,</span> <span class="n">vals</span><span class="p">):</span>
          <span class="n">X</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

  <span class="c1"># Compressed Sensing Column (CSC) matrix
</span>  <span class="k">return</span> <span class="n">sps</span><span class="p">.</span><span class="nf">csc_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calc_errors_pctile</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">Y_backup</span><span class="p">,</span> <span class="n">pct</span><span class="p">):</span>
  <span class="n">error_list</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_backup</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">error_list</span><span class="p">,</span> <span class="n">pct</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D_generating</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">k_codewords</span> <span class="o">=</span> <span class="n">D</span><span class="p">.</span><span class="n">shape</span>

  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k_codewords</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k_codewords</span><span class="p">):</span>
          <span class="k">if</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">D</span><span class="p">[:,</span><span class="n">j</span><span class="p">]).</span><span class="nf">transpose</span><span class="p">()</span><span class="o">*</span><span class="n">D_generating</span><span class="p">[:,</span><span class="n">k</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
              <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
              <span class="k">break</span>

  <span class="k">return</span> <span class="n">correct</span>

<span class="k">def</span> <span class="nf">calc_ksvd</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_sparsity</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">pct</span> <span class="o">=</span> <span class="mi">95</span><span class="p">):</span>
  <span class="sh">'''</span><span class="s">
  Arguments:

      Y: 2-D matrix, shape = (n_features, n_samples)
          Original signals, each column represents a signal.

      n_atoms: int
          Number of signal atoms in the dictionary D.

      n_sparsity: int
          Maximal number of non-zero elements allowed in each measurment vector.

      max_iter: int
          Maximal number of iterations allowed.

      tol: float
          Tolerance of convergence measured as L-2 norm ||y-Dx||_2 for one signal.

      pct: float
          Percentile of norm errors over signals. Convergence is reached if the
          pct-th percentile of signal norm error is less than tol.

  Returns:

      D: 2-D matrix, shape = (n_features, n_atoms)
          Dictionary of signal atoms.

      X: 2-D sparse matrix, shape = (n_atoms, n_samples)
          Each column contains the coefficients for sparse representation of
          signals.

      err_sparse_coding: dictionary(key:int, value:float)
          Dictionary with keys being iteration count, values being norm error
          after sparse coding.

      err_dict_update: dictionary(key:int, value:float)
          Dictionary with keys being iteration count, values being norm error
          after dictionary update.
  </span><span class="sh">'''</span>

  <span class="n">n_features</span><span class="p">,</span> <span class="n">n_samples</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="n">shape</span>

  <span class="c1"># Initialize dictionary
</span>  <span class="n">D_0</span> <span class="o">=</span> <span class="nf">gen_rand_dict_mat</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">)</span>
  <span class="n">iter_count</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">err_sparse_coding</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">err_dict_update</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="n">D</span> <span class="o">=</span> <span class="n">D_0</span>
  <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
      <span class="n">iter_count</span> <span class="o">=</span> <span class="n">iter_count</span> <span class="o">+</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="n">iter_count</span> <span class="o">&gt;</span> <span class="n">max_iter</span><span class="p">:</span>
          <span class="k">break</span>

      <span class="c1"># Sparse coding
</span>      <span class="n">omp</span> <span class="o">=</span> <span class="nc">OrthogonalMatchingPursuit</span><span class="p">(</span><span class="n">n_nonzero_coefs</span><span class="o">=</span><span class="n">n_sparsity</span><span class="p">)</span>
      <span class="n">omp</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
      <span class="n">X</span> <span class="o">=</span> <span class="n">sps</span><span class="p">.</span><span class="nf">csc_matrix</span><span class="p">(</span><span class="n">omp</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="nf">transpose</span><span class="p">())</span>
      <span class="n">err_sparse_coding</span><span class="p">[</span><span class="n">iter_count</span><span class="p">]</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="o">-</span><span class="n">D</span><span class="o">*</span><span class="n">X</span><span class="p">)</span>
      <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Iteration = {}/{}, Sparse Codeing - Columm-Pct Norm error = {}</span><span class="sh">"</span><span class="p">.</span>
            <span class="nf">format</span><span class="p">(</span><span class="n">iter_count</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="nf">calc_errors_pctile</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">D</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="n">pct</span><span class="p">)))</span>

      <span class="c1"># Codebook update
</span>      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">):</span>
          <span class="n">xk</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span>
          <span class="n">DX_k</span> <span class="o">=</span> <span class="n">D</span><span class="o">*</span><span class="n">X</span> <span class="o">-</span> <span class="n">D</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">xk</span>
          <span class="n">E</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">DX_k</span>

          <span class="n">nonzero_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">nonzero</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>
          <span class="n">nonzero_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">nonzero_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
          <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">nonzero_size</span><span class="p">)</span>
          <span class="n">Omega</span> <span class="o">=</span> <span class="n">sps</span><span class="p">.</span><span class="nf">csc_matrix</span><span class="p">((</span><span class="n">ones</span><span class="p">,</span> <span class="p">(</span><span class="n">nonzero_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nf">range</span><span class="p">(</span><span class="n">nonzero_size</span><span class="p">))),</span>
                                 <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">nonzero_size</span><span class="p">))</span>

          <span class="n">E_Omega</span> <span class="o">=</span> <span class="n">E</span> <span class="o">*</span> <span class="n">Omega</span>
          <span class="n">xk_Omega</span> <span class="o">=</span> <span class="n">xk</span> <span class="o">*</span> <span class="n">Omega</span>

          <span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="nf">svd</span><span class="p">(</span><span class="n">E_Omega</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="n">dk</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
          <span class="n">xk_Omega</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">Vh</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
          <span class="n">xk</span> <span class="o">=</span> <span class="n">sps</span><span class="p">.</span><span class="nf">csc_matrix</span><span class="p">((</span><span class="n">xk_Omega</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">nonzero_size</span><span class="p">),</span>
                               <span class="n">nonzero_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span>

          <span class="c1"># Update X_J
</span>          <span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">xk</span>
          <span class="c1"># Update D_J
</span>          <span class="n">D</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mat</span><span class="p">(</span><span class="n">dk</span><span class="p">).</span><span class="n">T</span>

      <span class="n">err_dict_update</span><span class="p">[</span><span class="n">iter_count</span><span class="p">]</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="o">-</span><span class="n">D</span><span class="o">*</span><span class="n">X</span><span class="p">)</span>
      <span class="n">err</span> <span class="o">=</span> <span class="nf">calc_errors_pctile</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">D</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="n">pct</span><span class="p">)</span>
      <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration = {}/{}, Dict Update    - Columm-Pct Norm error = {}</span><span class="sh">'</span><span class="p">.</span>
            <span class="nf">format</span><span class="p">(</span><span class="n">iter_count</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">err</span><span class="p">))</span>

      <span class="c1"># Termination condition
</span>      <span class="k">if</span> <span class="n">err</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
          <span class="k">break</span>

  <span class="k">return</span> <span class="n">D</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">err_sparse_coding</span><span class="p">,</span> <span class="n">err_dict_update</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

  <span class="c1"># Simulate test data
</span>  <span class="c1"># Y.shape = (n, N)
</span>  <span class="c1"># D.shape = (n, K)
</span>  <span class="c1"># X.shape = (K, N)
</span>  <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
  <span class="n">n</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">n_sparsity</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span>
  <span class="n">D</span> <span class="o">=</span> <span class="nf">gen_rand_dict_mat</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
  <span class="n">X</span> <span class="o">=</span> <span class="nf">gen_rand_measure</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">n_sparsity</span><span class="p">)</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="n">D</span> <span class="o">*</span> <span class="n">X</span>

  <span class="c1"># Run K-SVD and store results
</span>  <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">20</span>
  <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-2</span>
  <span class="n">pct</span> <span class="o">=</span> <span class="mi">95</span>
  <span class="n">result_D</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">result_X</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">result_errsc</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># error after sparse coding
</span>  <span class="n">result_errdu</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># error after dictionary update
</span>  <span class="k">for</span> <span class="n">i_sim</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
      <span class="n">D_J</span><span class="p">,</span> <span class="n">X_J</span><span class="p">,</span> <span class="n">err_sparse_coding</span><span class="p">,</span> <span class="n">err_dict_update</span> <span class="o">=</span> <span class="nf">calc_ksvd</span><span class="p">(</span>
        <span class="n">Y</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">n_sparsity</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">pct</span><span class="o">=</span><span class="n">pct</span><span class="p">)</span>
      <span class="n">result_D</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">D_J</span><span class="p">)</span>
      <span class="n">result_X</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">X_J</span><span class="p">)</span>
      <span class="n">result_errsc</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">err_sparse_coding</span><span class="p">)</span>
      <span class="n">result_errdu</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">err_dict_update</span><span class="p">)</span>

  <span class="c1"># Display results for a particular simulation
</span>  <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># the first simulation
</span>  <span class="n">D_J</span> <span class="o">=</span> <span class="n">result_D</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="n">X_J</span> <span class="o">=</span> <span class="n">result_X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="n">err_sparse_coding</span> <span class="o">=</span> <span class="n">result_errsc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="n">err_dict_update</span> <span class="o">=</span> <span class="n">result_errdu</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

  <span class="n">d_accuracy_tol</span> <span class="o">=</span> <span class="mf">0.07</span>
  <span class="n">success</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D_J</span><span class="p">,</span> <span class="n">d_accuracy_tol</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">With tolerance = {}. The number of success = {} out of {}</span><span class="sh">'</span><span class="p">.</span>\
    <span class="nf">format</span><span class="p">(</span><span class="n">d_accuracy_tol</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>

  <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
  <span class="n">error_list</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">D_J</span> <span class="o">*</span> <span class="n">X_J</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">norm_list</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">rel_err_list</span> <span class="o">=</span> <span class="n">error_list</span> <span class="o">/</span> <span class="n">norm_list</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">rel_err_list</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Relative Column Norm Errors</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Frequency of Samples</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Histogram of Column Errors</span><span class="sh">'</span><span class="p">)</span>

  <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">err_sparse_coding</span><span class="p">.</span><span class="nf">values</span><span class="p">()</span> <span class="o">/</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Sparse coding</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">err_dict_update</span><span class="p">.</span><span class="nf">values</span><span class="p">()</span> <span class="o">/</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Dictionary update</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Relative Norm Error</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Errors over Iteration</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
  <span class="nf">main</span><span class="p">()</span></code></pre></figure>]]></content><author><name>Victor Wang</name></author><category term="maths"/><category term="maths"/><summary type="html"><![CDATA[We provide an executive summary of the paper by Michal Aharon, Michael Elad and Alfred Bruckstein that proposes the novel K-SVD algorithm for designing overcomplete dictionaries for signals sparse representation. In the next sections, we firstly describe the mathematics of signals sparse representation, and then explain how the K-SVD algorithm works in solving the problem. Finally, we present some synthetic experiments to test our implementation of the K-SVD algorithm.]]></summary></entry><entry><title type="html">Mixed finite element method - solving a linear elasticity coupled with diffusion problem</title><link href="https://vicaws.github.io/blog/2018/mfe-diffusion/" rel="alternate" type="text/html" title="Mixed finite element method - solving a linear elasticity coupled with diffusion problem"/><published>2018-02-14T15:12:00+00:00</published><updated>2018-02-14T15:12:00+00:00</updated><id>https://vicaws.github.io/blog/2018/mfe-diffusion</id><content type="html" xml:base="https://vicaws.github.io/blog/2018/mfe-diffusion/"><![CDATA[<p>We apply the mixed finite element method (see, for example, Gatica<d-cite key="gatica2014simple"></d-cite>) to solve a linear elasticity coupled with diffusion problem. The weak formulations of the elasticity problem and the diffusion problem are bridged by a fixed-point equation, so that all unknowns can be solved simultaneously. Thereafter, to seek for numerical solution, we state the proper discretization, or Galerkin approximation, of the weak formulations as well as an iterative algorithm for the fixed-point equation. We also discuss the solvability of both continuous formulation and discrete numerical schemes. It turns out that the solving strategy, namely the composite of mixed finite element method and fixed-point iteration, can be well-posed and guarantee a solution to the proposed problem. Finally we implement the numerical scheme and compute the solutions.</p> <p>Since it requires intensive notations for function spaces to define weak formulations and discuss sovability, we summarize the notations of key function spaces in <a href="#appendix">Appendix</a> for reference.</p> <h2 id="linear-elasticity-with-diffusive-body-force">Linear elasticity with diffusive body force</h2> <p>Let \(\Omega\) be a bounded domain of \(\mathbb{R}^n\), \(n \geq 2\), with Lipschitz-continuous boundary \(\Gamma\). The linear elasticity problem targets to solve the displacement \(\mathbf{u}\) and the stress tensor \(\sigma\) of an elastic material occupying the region \(\Omega\), such that,</p> \[\sigma = \mathcal{C} \varepsilon (\mathbf{u}) \quad \text{in } \Omega,\] <p>\begin{equation} -\nabla \cdot \sigma = \mathbf{f} \quad \text{in } \Omega, \label{eq:linearElasticity} \end{equation}</p> <p>where \(\mathbf{f} \in [L^2(\Omega)]^n\) is the body force per unit volume, \(\varepsilon (\mathbf{u}) : = \frac{1}{2} \left[ \nabla \mathbf{u} + (\nabla \mathbf{u})^\top \right]\) is the strain tensor, and \(\mathcal{C}\) is the elasticity operator given by Hooke’s law, that is, \begin{equation} \mathcal{C} \zeta = \lambda \text{tr} (\zeta) \mathbf{I} + 2 \mu \zeta \quad \forall \zeta \in [L^2 (\Omega)]^{n \times n}. \end{equation} Here, \(\lambda, \mu&gt;0\) are the respective Lam'e constants, \(\mathbf{I}\) is the identity matrix of \(\mathbb{R}^n\), and tr(\(\cdot\)) is the matrix trace.</p> <p>We introduce diffusion term into the body force term by letting \(\mathbf{f} = \mathbf{f} (\phi)\), with \(\phi\) being the solution of a diffusion problem</p> <p>\begin{equation} -\nabla \cdot (D(\sigma) \nabla \phi) = g. \label{eq:diffusion} \end{equation}</p> <p>Here we assume \(g \in L^2(\Omega)\) and that the diffusivity \(D\) depends on the stress tensor \(\sigma\). As a result, the linear elasticity problem (\ref{eq:linearElasticity}) and the diffusion problem (\ref{eq:diffusion}) have to be solved simultaneously.</p> <p>We consider a simple set-up for the boundary conditions of both problems, where there are only Dirichlet boundaries. In particular, we let</p> <p>\begin{equation} \mathbf{u} = \mathbf{0} \quad \text{on } \Gamma, \label{eq:bcLE} \end{equation}</p> <p>\begin{equation} \phi = \phi_D \quad \text{on } \Gamma. \label{eq:bcD} \end{equation}</p> <h2 id="weak-formulation-and-solvability">Weak formulation and solvability</h2> <p>We derive the weak formulation for the linear elasticity problem (\ref{eq:linearElasticity}) and the diffusion problem (\ref{eq:diffusion}) by incorporating the Dirichlet boundary conditions (\ref{eq:bcLE}) and (\ref{eq:bcD}), and then state the solvability of the system of problems.</p> <p>\paragraph{Weak formulation of the linear elasticity problem}</p> <p>We introduce an auxiliary unknown \(\rho\) named rotation of the material by \begin{equation} \rho := \frac{1}{2} \left[ \nabla \mathbf{u} - (\nabla \mathbf{u})^\top \right], \end{equation}</p> <p>which by construction implies that \(\rho \in L^2\_\text{skew} (\Omega)\), where</p> <p>\begin{equation} L^2_\text{skew}(\Omega) := \Big\{ \eta \in [L^2(\Omega)]^{n\times n} : \eta + \eta^\top = 0 \Big\}. \end{equation}</p> <p>Thereafter, we obtain the following weak formulation:</p> <blockquote> <p>Find \((\sigma, (\mathbf{u}, \rho)) \in H \times Q\) such that \begin{equation} a(\sigma, \tau) + b(\tau, (\mathbf{u}, \rho)) = F(\tau) \forall \tau \in H \label{eq:weakLE0} \end{equation} \begin{equation} b(\sigma, (\mathbf{v}, \eta)) = G(\mathbf{v}, \eta) \forall (\mathbf{v}, \eta) \in Q, \end{equation} where the two function spaces \(H\) and \(Q\) are defined by \begin{equation} H := \{ \tau \in [L^2(\Omega)]^{n \times n} : \nabla \cdot \tau \in [L^2(\Omega)]^n, \int_\Omega \text{tr}(\tau) = 0 \}, \end{equation} \begin{equation} Q := [L^2(\Omega)]^n \times L^2_\text{skew}(\Omega), \end{equation} and the bilinear forms \(a:H\times H \mapsto \mathbb{R}\) and \(b:H\times Q \mapsto \mathbb{R}\) are defined by \begin{equation} a(\zeta, \tau) := \frac{1}{2\mu} \int_\Omega \Big( \zeta:\tau - \frac{\lambda}{n\lambda + 2\mu} \text{tr}(\zeta) \text{tr}(\tau) \Big), \end{equation} \begin{equation} b(\sigma, (\mathbf{v}, \eta)) := \int_\Omega \mathbf{v} \cdot \nabla \cdot \tau + \int_\Omega \eta : \tau, \end{equation} and the functionals \(F\) and \(G\) are given by \begin{align} F(\tau) := 0, \quad G(\mathbf{v}, \eta) := - \int_\Omega \mathbf{f} \cdot \mathbf{v}. \label{eq:weakLE} \end{align}</p> </blockquote> <h3 id="weak-formulation-of-the-diffusion-problem">Weak formulation of the diffusion problem</h3> <p>Since the boundary condition (\ref{eq:bcD}) is inhomogeneous, we set \(\phi = \hat{\phi} + \phi_D\) and solve \(\hat{\phi}\) instead. By letting \(\phi_D\) be a constant, we obtain the following weak formulation:</p> <blockquote> <p>Find \(\hat{\phi} \in V\) such that \begin{align} c(\hat{\phi},v) &amp; = L(v) &amp; \forall v \in V, \label{eq:weakD0} \end{align} where the function space \(V\) is defined by \begin{align} V &amp; := \Big\{ v \in H^1(\Omega) : v = 0 \text{ on } \Gamma \Big\}, \label{eq:defV} \end{align} and the bilinear forms \(L:V\times V \mapsto \mathbb{R}\) is defined by \begin{align} c(u, v) &amp; := \int_\Omega D(\sigma) \nabla \hat{\phi} \cdot \nabla v , \end{align} and the functional \(L\) is given by \begin{align} L(v) &amp; := \int_\Omega g v \label{eq:weakD} \end{align}</p> </blockquote> <h3 id="solving-strategy-for-the-system-of-problems">Solving strategy for the system of problems</h3> <p>Because the weak formulations of linear elasticity problem (\ref{eq:weakLE0})-(\ref{eq:weakLE}) and the diffusion problem (\ref{eq:weakD0})-(\ref{eq:weakD}) interact with each other, all the unknowns have to be solved simultaneously. To solve them, we define a fixed-point problem:</p> <blockquote> <p>We fix a solution of the diffusion problem (\ref{eq:weakD}), \(\hat{\phi}\), and define the solution operator for the linear elasticity problem (\ref{eq:weakLE}) as \begin{align} \mathcal{S}_E (\hat{\phi}) = (\sigma, (\mathbf{u}, \rho)) \quad \forall \hat{\phi} \in V. \label{eq:fixpoint0} \end{align} Similarly, we fix a solution of the linear elasticity problem, \((\sigma, (\mathbf{u}, \rho))\), and define the solution operator for the diffusion problem as \begin{align} \mathcal{S}_D (\sigma, (\mathbf{u}, \rho)) = \hat{\phi} \quad \forall (\sigma, (\mathbf{u}, \rho)) \in H \times Q. \end{align} Finally, define \(T: V \mapsto V\) by \begin{align} T(\hat{\phi}) = \mathcal{S}_D (\mathcal{S}_E (\hat{\phi})) \quad \forall \hat{\phi} \in V, \end{align} such that our goal is to solve the following fixed-point equation \begin{align} T(\hat{\phi}) = \hat{\phi}. \label{eq:fixpoint} \end{align}</p> </blockquote> <h3 id="discussion-on-the-solvability">Discussion on the solvability</h3> <p>The solvability of the problem (\ref{eq:fixpoint0})-(\ref{eq:fixpoint}) depends on the solvability of three subproblems, namely the linear elasticity problem (\ref{eq:weakLE}) given \(\hat{\phi}\), the diffusion problem (\ref{eq:weakD}) given \((\sigma, (\mathbf{u}, \rho))\), and eventually the fixed-point equation (\ref{eq:fixpoint}).</p> <p>We start from the easier diffusion problem. The Lax-Milgram theorem guarantees the existence and uniqueness of the solution of (\ref{eq:weakD}) because we can show that</p> <ul> <li> <p>\(V\) defined in (\ref{eq:defV}) is a closed subspace of \(H^1(\Omega)\), and \(H^1(\Omega)\) with the norm \(\|v\|^2_{1,\Omega} := \|v\|^2_{L^2(\Omega)} + \|\nabla v\|^2_{L^2(\Omega)}\) is a Hilbert space;</p> </li> <li> <p>\(c(\cdot, \cdot)\) is coercive on \(H^1(\Omega) \times H^1(\Omega)\) (Cauchy-Schwarz inequality);</p> </li> <li> <p>\(c(\cdot, \cdot)\) is bounded on \(H^1(\Omega) \times H^1(\Omega)\) (Poincar'e inequality);</p> </li> <li> <p>\(L(\cdot)\) is bounded on \(H^1(\Omega)\) (Cauchy-Schwarz inequality).</p> </li> </ul> <p>The linear elasticity problem involves mixed formulations, where Lax-Milgram theorem needs to be adapted to apply for demonstrating the solvability. In summary, we can prove the well-posedness of (\ref{eq:weakLE}) by showing that</p> <ul> <li> <p>\(H\) and \(Q\) are Hilbert spaces;</p> </li> <li> <p>\(a(\cdot, \cdot)\) is bounded on \(H \times H\), \(b(\cdot, \cdot)\) is bounded on \(H \times Q\);</p> </li> <li> <p>\(F(\cdot)\) is bounded on \(H\), \(G(\cdot)\) is bounded on \(Q\);</p> </li> <li> <p>\(a(\cdot, \cdot)\) is coercive on \(K\) which is defined by</p> </li> </ul> <p>\begin{equation} K := { \tau \in H : b(\sigma, (\mathbf{v}, \eta)) = 0 \quad \forall (\mathbf{v}, \eta) \in Q }; \label{eq:kerlB} \end{equation} \item \(b(\cdot, \cdot)\) satisfies the inf-sup condition, that is, \(\exists c_b &gt;0\) such that \begin{equation} c_b \leq \inf_{(\mathbf{v}, \eta) \in Q \backslash { (\mathbf{0}, \mathbf{0}) }} \sup_{\tau \in H \backslash {\mathbf{0}}} \frac{b(\tau, (\mathbf{v}, \eta))}{||\tau||_H ||(\mathbf{v}, \eta)||_Q} ; \end{equation}</p> <p>The last piece is to show the fixed-point equation (\ref{eq:fixpoint}) yields a root. In fact, fixed-point theorem confirms the existence of a solution if we show that \(T(\cdot)\) is continuous from a solution space \(W\) into itself, and that \(T(W)\) is a relatively compact subspace of \(W\). To define \(W\), we recall that the well-posedness of the diffusion problem also indicates that \(\|\hat\phi\|_{1,\Omega} \leq C \|L\|_{H^1(\Omega)'} \leq C_{\phi}\) for some constant \(C_\phi\). Thereafter, we can define the solution space for \(\hat{\phi}\) as</p> <p>\begin{equation} W = \Big\{ v \in H^1(\Omega) : ||v||_{1,\Omega} \leq C_\phi \Big\}. \end{equation}</p> <p>Hence, we show that</p> <ul> <li> <p>\(T(W) \subset W\);</p> </li> <li> <p>The continuity of \(T\), that is, \(\forall \phi, \varphi \in W\)</p> </li> </ul> <p>\begin{equation} ||T(\phi) - T(\varphi)||_{1,\Omega} \leq C ||\phi - \varphi||_{1,\Omega}. \end{equation}</p> <h2 id="discrete-formulation---galerkin-approximation-and-fixed-point-iteration">Discrete formulation - Galerkin Approximation and Fixed-Point Iteration</h2> <p>Suppose \(H_h \subset H\), \(Q_h \subset Q\) and \(V_h \subset V\) are linear subspaces of the corresponding Hilbert spaces. The discrete Galerkin approximation of the linear elasticity problem weak formulation (\ref{eq:weakLE0})-(\ref{eq:weakLE}) is:</p> <blockquote> <p>Find \((\sigma_h, (\mathbf{u}_h, \rho_h)) \in H_h \times Q_h\) such that \begin{align} a(\sigma_h, \tau_h) + b(\tau_h, (\mathbf{u}_h, \rho_h)) = F(\tau_h) \text{ }\forall \tau_h \in H_h \label{eq:galerkinLE0} \end{align} \begin{align} b(\sigma_h, (\mathbf{v}_h, \eta_h)) = G(\mathbf{v}_h, \eta_h) \quad \forall (\mathbf{v}_h, \eta_h) \in Q_h. \label{eq:galerkinLE} \end{align}</p> </blockquote> <p>And the Galerkin approximation of the diffusion problem weak formulation (\ref{eq:weakD0})-(\ref{eq:weakD}) is:</p> <blockquote> <p>Find \(\hat{\phi}_h \in V_h\) such that \begin{align} c(\hat{\phi}_h,v_h) &amp; = L(v_h) &amp; \forall v_h \in V_h. \label{eq:galerkinD} \end{align}</p> </blockquote> <p>Finally, we solve the fixed-point equation using the following iterative method:</p> <blockquote> <p><strong>Algorithm</strong> Fixed-Point Iteration:</p> <p>     <strong>initialize</strong> \(\phi_h^0\), \(k=0\)</p> <p>     <strong>iterate</strong> until <code class="language-plaintext highlighter-rouge">res</code> \(&lt;\) <code class="language-plaintext highlighter-rouge">tol</code></p> <blockquote> <p>update \(k = k+1\);</p> <p>solve (\ref{eq:galerkinLE0})-(\ref{eq:galerkinLE}) using \(\phi_h^{k-1}\) \(\rightarrow\) \((\sigma_h^k, (\mathbf{u}_h^k, \rho_h^k))\) ;</p> <p>solve (\ref{eq:galerkinD}) using \(\sigma_h^{k-1}\) \(\rightarrow\) \(\phi_h^k\) ;</p> <p>define <code class="language-plaintext highlighter-rouge">res</code> = \(\| \phi_h^{k-1} - \phi_h^{k}\|_{L^\infty} + \| \sigma^{k-1} - \sigma^{k}\|_{L^\infty}\);</p> </blockquote> <p>     <strong>return</strong> \(\phi_h^k\), \((\sigma_h^k, (\mathbf{u}_h^k, \rho_h^k))\)</p> </blockquote> <h3 id="discussion-on-the-solvability-1">Discussion on the solvability</h3> <p>The fixed-point theorem guarantees that the iteration algorithm can find a root. It remains to show the solvability of the Galerkin approximation for two problems (\ref{eq:galerkinLE0})-(\ref{eq:galerkinLE}) and (\ref{eq:galerkinD}).</p> <p>For the diffusion problem (\ref{eq:galerkinD}), because \(c(\cdot, \cdot)\) and \(F(\cdot)\) satisfy the conditions of the Lax-Milgram Theorem, it can be shown that the Galerkin approximation is well-posed for any closed subspace \(V_h \subset V\). Moreover, C'ea’s lemma further states that the approximation error \(\|\hat{\phi} - \hat{\phi}\_h\|_V\) is bounded in terms of the best approximation errors \(\inf_{v_h \in V_h} \|\hat{\phi} - v_h\|_V\).</p> <p>The discrete mixed formulation (\ref{eq:galerkinLE0})-(\ref{eq:galerkinLE}) of the linear elasticity problem is not granted solvability despite all the conditions satisfied in the continuous problem. We additionally restrict the choice of Galerkin subspaces so that the following conditions shall be met</p> <ul> <li> <p>\(a(\cdot, \cdot)\) is coercive on \(K_h\) which is defined by \begin{equation} K_h := { \tau_h \in H_h : b(\sigma_h, (\mathbf{v}_h, \eta_h)) = 0 \text{ } \forall (\mathbf{v}_h, \eta_h) \in Q_h }; \label{eq:kerlBh} \end{equation}</p> </li> <li> <p>\(b(\cdot, \cdot)\) satisfies the inf-sup condition adapted to the subspaces \((H_h, Q_h)\), that is, \(\exists c_b &gt;0\) such that \begin{equation} c_b \leq \inf_{(\mathbf{v}_h, \eta_h) \in Q_h \backslash { (\mathbf{0}, \mathbf{0}) }} \sup_{\tau_h \in H_h \backslash {\mathbf{0}}} \frac{b(\tau_h, (\mathbf{v}_h, \eta_h))}{||\tau_h||_H ||(\mathbf{v}_h, \eta_h)||_Q}; \end{equation}</p> </li> </ul> <p>Thereafter, we can conclude that there is a unique solution pair \((\sigma_h, (\mathbf{u}\_h, \rho_h)) \in H_h \times Q_h\) to (\ref{eq:galerkinLE}). Moreover, the error \(\|\sigma - \sigma_h\|_H + \|(\mathbf{u}, \rho)-(\mathbf{u}\_h, \rho_h)\|_Q\) is bounded in terms of the best approximation error \(\inf_{\tau_h \in H_h} \|\sigma - \tau_h\|_H\) and \(\inf_{(\mathbf{v}_h, \tau_h) \in Q_h} \|(\mathbf{u}-\rho) - (\mathbf{v}_h, \tau_h)\|_Q\).</p> <h2 id="numerics">Numerics</h2> <p>We show an implementation of solving a two-dimension linear elasticity with diffusive body force problem using <code class="language-plaintext highlighter-rouge">FEniCS</code> package. In Table 1, we list all parameters and their values assigned in the implementation.</p> <table> <thead> <tr> <th style="text-align: center"><strong>Param</strong></th> <th><strong>Description</strong></th> <th><strong>Value</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(n\)</td> <td>Dimension</td> <td>\(2\)</td> </tr> <tr> <td style="text-align: center">\(\Omega\)</td> <td>Domain</td> <td>\((x,y) \in ([0, 40], [0, 40])\)</td> </tr> <tr> <td style="text-align: center">\((\lambda, \mu)\)</td> <td>Lame constants</td> <td>\((4, 1)\)</td> </tr> <tr> <td style="text-align: center">\(\mathbf{f}(\phi)\)</td> <td>Body force</td> <td>\((\sin 2\pi x, 0) \phi\)</td> </tr> <tr> <td style="text-align: center">\(D(\sigma)\)</td> <td>Diffusivity</td> <td>\(\mathbf{I} + 0.01*\sigma\)</td> </tr> <tr> <td style="text-align: center">\(g\)</td> <td>Diffusion source</td> <td>0.01</td> </tr> <tr> <td style="text-align: center">\(\phi_D\)</td> <td>Diffusion boundary</td> <td>1</td> </tr> <tr> <td style="text-align: center">FE for \(H_h\)</td> <td>Finite element</td> <td>BDM</td> </tr> <tr> <td style="text-align: center">FE for \(Q_h\)</td> <td>Finite element</td> <td>DG</td> </tr> <tr> <td style="text-align: center">FE for \(V_h\)</td> <td>Finite element</td> <td>CG</td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">tol</code></td> <td>Residual tolerance</td> <td><code class="language-plaintext highlighter-rouge">5e-4</code></td> </tr> </tbody> </table> <p><strong>Table 1</strong> - List of Parameters and Values</p> <p>The solution for the displacement field \(\mathbf{u}\) is shown in Figure 1, where magnitudes of \(x\)-component and \(y\)-component are displayed separately.</p> <p>In Figiure 2, we show the magnitudes of four components of the 2-dimensional stress tensor \(\sigma\). Note that \(\sigma\) is symmetric, verified by the same solution obtained for \(\sigma_{12}\) and \(\sigma_{21}\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/mfe-diffusion/u.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 1</strong> - Solution of \(\mathbf{u}\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/mfe-diffusion/sigma.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 2</strong> - Solution of \(\sigma\).</p> <h2 id="appendix">Appendix</h2> <p>Let \(\Omega\) be a bounded domain of \(\mathbb{R}^n\), \(n \geq 2\), with Lipschitz-continuous boundary \(\Gamma\). Here we summarise the definitions of relevant functions spaces \(\{u: \Omega \mapsto \mathbb{R} \}\) and their notations used throughout the paper.</p> <h3 id="hilbert-spaces">Hilbert spaces</h3> <p>A Hilbert space \(H(\Omega)\) is a complete inner product space. Completeness means that all Cauchy sequences in \(H(\Omega)\) converge in \(H(\Omega)\).</p> <h3 id="lebesgue-spaces">Lebesgue spaces</h3> <p>For \(p \in [1, \infty]\), the \(L^p (\Omega)\) space is defined by</p> \[L^p(\Omega) = \left\{ u: \|u\|_{L^p(\Omega)} &lt; \infty \right\},\] <p>where the \(L^p\) norm is defined as</p> \[\|u\|_{L^p(\Omega)} = \left( \int_\Omega |u|^p \text{d}x \right)^{1/p} \quad p \in [1, \infty),\] \[\|u\|_{L^\infty(\Omega)} = \inf \left\{ C\geq 0: |u(x)| \leq C, \forall x \in \Omega \right\}.\] <p>\(L^2\) space is also a Hilbert space, namely, \(L^2(\Omega) = H(\Omega)\).</p> <h3 id="sobolev-spaces">Sobolev spaces</h3> <p>The Sobolev space \(W^k_p(\Omega)\), with non-negative integer \(k\) and \(p \in [1, \infty]\) is defined by</p> \[W^k_p(\Omega) = \left\{ u \in L_{loc}^1(\Omega) : \|u\|_{W^k_p(\Omega) &lt; \infty} \right\},\] <p>where \(L_{loc}^1(\Omega)\) denotes the set of locally integrable functions, and the Sobolev norm is defined as</p> \[\|u\|_{W^k_p(\Omega)} = \left( \sum_{|\alpha| \leq k} \|D^\alpha u\|_{L^p(\Omega)}^p \right)^{1/p} \quad p \in [1, \infty),\] \[\|u\|_{W^k_\infty(\Omega)} = \max_{|\alpha| \leq k} \|D^\alpha u\|_{L^\infty(\Omega)}.\] <p>Here \(D^\alpha u\) denotes \(\alpha\)-th order of weak derivatives of \(u\).</p> <h3 id="sobolev-spaces-1">Sobolev spaces</h3> <p>The Sobolev spaces with \(p=2\) are Hilbert spaces. These are denoted by</p> \[H^k(\Omega) = W_2^k (\Omega).\] <p>Moreover, if we ask for no weak derivatives, the Sobolev space becomes \(L^p\) space, that is, \(W^0_p(\Omega) = L^p(\Omega)\). This in consequence leads to \(H^0(\Omega) = L^2(\Omega)\).</p>]]></content><author><name>Victor Wang</name></author><category term="maths"/><category term="maths"/><summary type="html"><![CDATA[We apply the mixed finite element method (see, for example, Gatica) to solve a linear elasticity coupled with diffusion problem. The weak formulations of the elasticity problem and the diffusion problem are bridged by a fixed-point equation, so that all unknowns can be solved simultaneously. Thereafter, to seek for numerical solution, we state the proper discretization, or Galerkin approximation, of the weak formulations as well as an iterative algorithm for the fixed-point equation. We also discuss the solvability of both continuous formulation and discrete numerical schemes. It turns out that the solving strategy, namely the composite of mixed finite element method and fixed-point iteration, can be well-posed and guarantee a solution to the proposed problem. Finally we implement the numerical scheme and compute the solutions.]]></summary></entry><entry><title type="html">The Graph Coloring Problem (GCP)</title><link href="https://vicaws.github.io/blog/2017/graph-coloring/" rel="alternate" type="text/html" title="The Graph Coloring Problem (GCP)"/><published>2017-12-10T16:00:00+00:00</published><updated>2017-12-10T16:00:00+00:00</updated><id>https://vicaws.github.io/blog/2017/graph-coloring</id><content type="html" xml:base="https://vicaws.github.io/blog/2017/graph-coloring/"><![CDATA[<p>The graph coloring problem (GCP) refers to the problem of finding the coloring of an undirected, unweighted graph with the minimal number of colors (namely, the chromatic number). To solve the GCP problem, we implement the Greedy Coloring (GC) algorithm and the Recursive Largest First (RLF) algorithm. Moreover, we also formulate the GCP problem into an Integer Programming (IP) problem and attempt to solve it. Finally, the performances of the algorithms are compared in terms of optimality and computational time.</p> <h2 id="heuristic-algorithms">Heuristic algorithms</h2> <p>We will describe two heuristic algorithms for the GCP problem, namely the Greedy Coloring (GC) algorithm and the Recursive Largest First (RLF) algorithm. The GC iterates over vertices of the graph, and tries to use existing colors as much as possible provided that all neighbors have not occupied all. In contrast, the RLF iterates over colors, and in each iteration it tries to assign the color to as many vertices as possible.</p> <p>Before walking into the algorithms we introduce the notations used throughout the paper. Consider a graph \(G = (V,E)\) with vertex set \(V = \{ v_1, \cdots, v_n \}\) and edge set \(E\). The color function \(\phi(\cdot)\) maps from a vertex to a color. For simplicity, and w.l.o.g., we set the color set to be the set of positive natural numbers \(\mathbb{N}_+\).</p> <h3 id="greedy-coloring-gc">Greedy Coloring (GC)</h3> <p>The GC algorithm is stated in the following box.</p> <blockquote> <p><strong>function</strong> Greedy-Coloring (graph \(G = (V, E)\)):</p> <p>     <strong>initialise</strong> set \(\phi(v_1) = 1\) and \(\phi(v_i)=0\) for all \(2 \leq i \leq n\)</p> <p>     <strong>iterate</strong> \(2 \leq i \leq n\)</p> <blockquote> <p>let \(\phi(v_i) = \min \{ k \in \mathbb{N}_+ \\| \phi(v_j) \neq k\) if \(v_i v_j \in E \}\);</p> </blockquote> <p><strong>return</strong> \(\phi(v)\)</p> </blockquote> <p>Thus, the GC algorithm ends up with a \(k\)-coloring where</p> <p>\begin{equation} k = \max_{v_i \in V} \phi (v_i). \end{equation}</p> <p>Note that the GC algorithm aims for optimality by assigning the smallest possible color to a vertex in one iteration.</p> <p>One important feature of the GC algorithm is that the pre-defined order of vertices completely decides the order of being colored. Different orders of being colored might result in different minimal numbers of colors. In other words, the optimality of the GC algorithm is not guaranteed, and is sensitive to the order of vertices to be colored.</p> <h3 id="recursive-largest-first-rlf">Recursive Largest First (RLF)</h3> <p>Unlike GC that iterates over vertices, the RLF algorithm iterates over colors and maximises the number of vertices that can be assigned a color in each iteration. The algorithm details are shown in the following box.</p> <blockquote> <p><strong>function</strong> RLF (graph \(G = (V,E)\)):</p> <p>     <strong>initialise</strong> calculate the degree for each vertex, select the vertex with the largest degree for coloring. The largest degree vertex is assigned the <code class="language-plaintext highlighter-rouge">active_color = 1</code>. Define <code class="language-plaintext highlighter-rouge">init_vertex</code> as the colored vertex;</p> <p>     <strong>iterate</strong> until all vertices are colored</p> <blockquote> <p>find all adjacent vertices of the <code class="language-plaintext highlighter-rouge">init_vertex</code> and add to a set \(A\);</p> <p>find all vertices that are not adjacent to the <code class="language-plaintext highlighter-rouge">init_vertex</code> and add to a set \(U\);</p> <p>     <strong>iterate</strong> until \(U\) is empty</p> <blockquote> <p>calculate the number of adjacent vertices which are in \(A\) for each vertex in \(U\). After that, the uncolored vertex with maximum adjacent vertices (which are in \(A\)) in \(U\) is selected for coloring. The selected vertex is colored with the <code class="language-plaintext highlighter-rouge">active_color</code>;</p> <p>remove the colored vertex and its adjacent vertices from \(U\), and add them to \(A\);</p> </blockquote> <p>update <code class="language-plaintext highlighter-rouge">active_color = active_color + 1</code>;</p> <p>calculate the number of adjacent vertices which are uncolored for every uncolored vertex. After that, the uncolored vertex having maximum uncolored adjacent vertices is selected for coloring process. If more than one vertex satisfies this condition, the vertex with the largest degree among them is selected. Color the selected vertex with <code class="language-plaintext highlighter-rouge">active_color</code>, and update <code class="language-plaintext highlighter-rouge">init_vertex</code> with the colored vertex;</p> </blockquote> <p><strong>return</strong> color assignment</p> </blockquote> <p>The primary idea of the RLF algorithm is to assign one color to as many vertices as possible and then move to assigning the next color. More than that, in one color assignment, it defines the order of vertices to be colored basically according to vertex degree. Vertex with larger degree has higher priority of being colored. We expect that RLF shall be much less sensitive to the initial ordering of vertices than GC.</p> <h2 id="integer-programming-formulation">Integer programming formulation</h2> <p>We can formulate the GCP into an integer programming (IP) problem. Thereafter, solving the IP problem results in the chromatic number.</p> <p>In a graph with vertex set \(V = \{ v_1, \cdots, v_n \}\), we need at most \(n\) colors to color all the vertices. We introduce binary variables \(y_k\) for \(k=1,\cdots,n\) such that</p> \[y_k = \begin{cases} 1, \text{ color } k \text{ is used}, \\ 0, \text{ color } k \text{ is not used}. \end{cases}\] <p>Furthermore, we introduce binary variables \(x_{ik}\) to indicate whether vertex \(i\) is assigned color \(k\), for \(i,k=1,\cdots,n\)</p> \[x_{ik} = \begin{cases} 1, \text{ vertex } i \text{ is assigned color } k, \\ 0, \text{ vertex } i \text{ is not assigned color } k. \end{cases}\] <p>The resulting IP is</p> <p>\begin{align} \min_{x,y} \quad &amp; \sum_{j=1}^n y_k \label{eq:IP_obj} <br/> \end{align} \begin{align} \text{s.t.} \quad &amp; \sum_{k=1}^n x_{ik} = 1, &amp; \forall i = 1,\cdots,n; \label{eq:IP_1to1} <br/> \end{align} \begin{align} &amp; x_{ik} + x_{jk} \leq 1, &amp; \forall v_i v_j \in E, k = 1,\cdots,n; \label{eq:IP_noNeighbor} <br/> \end{align} \begin{align} &amp; x_{ik} - y_k \leq 0, &amp; \forall i,k = 1,\cdots,n; \label{eq:IP_relation} <br/> \end{align} \begin{align} &amp; 0 \leq x_{ik}, y_k \leq 1, &amp; \forall i,k = 1,\cdots,n; \label{eq:IP_bin1} <br/> \end{align} \begin{align} &amp; x_{ik}, y_k \in \mathbb{Z}, &amp; \forall i,k = 1,\cdots,n. \label{eq:IP_bin2} \end{align}</p> <p>The objective \eqref{eq:IP_obj} is to minimise the number of colors to be used. Constraint \eqref{eq:IP_1to1} addresses that each vertex is assigned one and only one color. The next constraint \eqref{eq:IP_noNeighbor} then forbids neighbor vertices from being assigned the same color. There should then be a constraint concerning the relationship between \(x_{ik}\) and \(y_k\). Constraint \eqref{eq:IP_relation} does the job: if color \(k\) is not used, then no vertices shall receive the color \(k\). The last two constraints \eqref{eq:IP_bin1} and \eqref{eq:IP_bin2} corresponds to the binary property of variables \(x\) and \(y\).</p> <p>The IP formulation of the GCP has \(\mathcal{O}(n^2) \sim \mathcal{O}(n^3)\) number of constraints, depending on the completeness of the graph. We can expect the solving complexity to increase dramatically as the graph size \(n\) grows.</p> <h2 id="case-study---petersen-graph">Case study - Petersen graph</h2> <p>We apply the two heuristic graph algorithms GC and RLF, and the IP approach to the Petersen graph, as shown in Figure 1.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/graph-coloring/Petersen_Graph.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 1</strong> - Petersen graph.</p> <p>We report the results from the three algorithms in \Cref{tab:PerformPetersen}. All three algorithms agree on the minimal number of colors, namely 3. In terms of computational speed, IP is the slowest. GC is slightly faster than RLF.</p> <table> <thead> <tr> <th><strong>Algorithm</strong></th> <th><strong>\(k\)-coloring</strong></th> <th><strong>Time (seconds)</strong></th> </tr> </thead> <tbody> <tr> <td>GC</td> <td>3</td> <td>0.0002</td> </tr> <tr> <td>RLF</td> <td>3</td> <td>0.0005</td> </tr> <tr> <td>IP</td> <td>3</td> <td>0.1476</td> </tr> </tbody> </table> <p><strong>Table 1</strong> - Algorithm performance on the Petersen graph.</p> <p>The LP relaxation problem of the IP can be obtained by removing the integral constraint (\ref{eq:IP_bin2}). Solving the LP relaxation problem, we get a result:</p> \[\begin{equation} y_{k} = \begin{cases} 0.5, &amp;\quad k =1,2,\\ 0, &amp;\quad k = 3,\cdots,n, \end{cases} \label{eq:LP_y} \end{equation}\] \[\begin{align} x_{ik} &amp; = \begin{cases} 0.5, &amp;\quad k=1,2, \text{ }\forall i, \\ 0, &amp;\quad k = 3,\cdots,n, \text{ }\forall i, \end{cases} \label{eq:LP_x} \end{align}\] <p>where \(n=10\) for the Petersen graph. Thus, the optimal objective value of the LP relaxation problem is 1, which is quite different from the optimum of the IP. In fact, this optimal solution is the optimal solution for LP relaxation problems of any graph. If forgetting the graph scale \(n\) for a moment, the only constraint concerning the topology of a graph is (\ref{eq:IP_noNeighbor}) that specifies no neighbor vertices shall receive the same color. Regardless of how edges are configured in \(E\), the optimal solution (\ref{eq:LP_x}) is always feasible under constraint (\ref{eq:IP_noNeighbor}). We can argue for the optimality of the solution by proof of contradiction.</p> <h2 id="case-study---erdos-renyi-graph">Case study - Erdos-Renyi graph</h2> <p>To study the scaling of the three algorithms to the GCP, we implement them to the Erdos-Renyi graph \(G(n,p)\) for varying \(n\) and fixed probability \(p=0.7\).</p> <p>Since the Erdos-Renyi graph is a random graph, we run each algorithm for 100 realisations and compare among algorithms by average performance and its standard deviation. The performance to be compared includes the minimal number of colors and computational time.</p> <p>Firstly, we compare the two heuristic algorithms. The average minimal number of colors achieved is shown in Figure 2. \(x\)-axis indicates the scale of graph \(n\), and \(y\)-axis marks the number of colors. The solid line represents the average minimal number of colors achieved. Furthermore, the error bar is also attached on the solid line, implying the range of two standard deviations (where 95% of data will fall within assuming that data follow normal distribution).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/graph-coloring/GC_RLF_chrom.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 2</strong> - Minimal number of colors achieved.</p> <p>Apparently RLF performs better and better than the greedy algorithm in finding the minimal number of colors as graph is getting larger and larger. In particular, in a \(G(200, 0.7)\) graph, GC says it requires on average 123.28 colors to color the graph, while RLF says 46.03 is enough. The standard deviations of results from both algorithms are relatively small, indicating that both algorithms are stable.</p> <p>Figure 3 then shows the average computational time comparison between the two algorithms. The computational time of both GC and RLF increases nonlinearly with \(n\). It is observed that RLF becomes faster then GC when the graph has more then 100 vertices. Indeed, though it is not quite observable in Figure 3, that GC is slightly faster than RLF when the graph has no more than 90 vertices. The performance table for the Petersen graph (Table 1) agrees with this observation, where GC elapsed in 0.0002 seconds while RLF elapsed in 0.0005 seconds. Anyway, the slight outperformance of GC applied in small-scaled graph seems negligible. The standard deviations of results from both algorithms are relatively small, indicating that both algorithms are stable.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/graph-coloring/GC_RLF_time.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 3</strong> - Computational time.</p> <p>Note that, all curves in Figure 2 and Figure 3 look relatively smooth and trendy. This shows that 100 realisations are a proper and sufficient choice to assess the algorithms performance for random graphs.</p> <p>We add the performance of IP to comparison. However, in this comparison, we are unable to increase the graph size to 200 as we did for the heuristic algorithms. The reason is that the computational time of IP increases dramatically as graph gets larger. In our numerical experiments, we use 100 different random generator seeds to control realisations. The IP can produce results within reasonably stable and short time period (\(\leq 5\) secs) for each realisation at small \(n\). However, when increasing \(n\) to 15, it takes around 20 mins to get results for some realisation but takes only seconds for others. This implies that the speed and stability of the IP becomes very sensitive to graph topology as graph sizes grow.</p> <p>In Figure 4, we show the average minimal number of colors achieved by each approach. Due to the instability of IP we observed when \(n \geq 15\), we only run 10 realisations (which exclude those requiring unaffordably long elapsed time) for graphs with more than 14 vertices. This causes the non-smooth behaviours of all curves for \(n \geq 15\).</p> <p>Since IP will always give the chromatic number, we use IP as the benchmark to show how far away each heuristic algorithm is from the optimality. From Figure 4, we see that RLF agrees with IP when \(n \leq 22\), and starts to produce less optimal result when graph size grows larger. GC’s result starts to diverge from IP’s results significantly since \(n \geq 9\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/graph-coloring/GC_RLF_IP_chrom.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 4</strong> - Minimal number of colors achieved.</p> <p>Looking at the average computational time, as shown in Figure 5, IP becomes much more expensive than the other two heuristic algorithms. What’s worse, as addressed before, IP requires 20 mins for solving some realisation of \(G(15,0.7)\). Figure 5 turns out only to show some “fast” realisations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/graph-coloring/GC_RLF_IP_time.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 5</strong> - Computational time.</p> <h2 id="conclusion">Conclusion</h2> <p>We have implemented the Greedy Coloring (GC) algorithm, the Recursive Largest First (RLF) algorithm, and Integer Programming (IP) to solve the graph coloring problem. In particular, we compare these 3 algorithms by the minimal number of colors achieved as well as computational time. The GC and RLF algorithms are heuristic and do not guarantee the minimal number of colors achieved is the chromatic number. RLF beats GC more and more when the graph size scales up, in terms of both optimality and speed. Integer programming always gives the chromatic number, as long as the formulated IP can be solved at optimality. However, the computational time of the IP approach can be unaffordable as the graph size scales up.</p>]]></content><author><name>Victor Wang</name></author><category term="maths"/><category term="maths"/><summary type="html"><![CDATA[The graph coloring problem (GCP) refers to the problem of finding the coloring of an undirected, unweighted graph with the minimal number of colors (namely, the chromatic number). To solve the GCP problem, we implement the Greedy Coloring (GC) algorithm and the Recursive Largest First (RLF) algorithm. Moreover, we also formulate the GCP problem into an Integer Programming (IP) problem and attempt to solve it. Finally, the performances of the algorithms are compared in terms of optimality and computational time.]]></summary></entry><entry><title type="html">Estimating Hidden Markov Models (HMM)</title><link href="https://vicaws.github.io/blog/2017/hmm/" rel="alternate" type="text/html" title="Estimating Hidden Markov Models (HMM)"/><published>2017-12-01T15:12:00+00:00</published><updated>2017-12-01T15:12:00+00:00</updated><id>https://vicaws.github.io/blog/2017/hmm</id><content type="html" xml:base="https://vicaws.github.io/blog/2017/hmm/"><![CDATA[<p>The Hidden Markov Model (HMM) assumes the system being modelled to be Markov process with unobserved (i.e. hidden) states. In particular, one problem is to learn the hidden Markov transition matrix and conditional probability on Markov states from a sequence of observations. We explain the Baum-Welch algorithm which finds the maximum likelihood estimate of these parameters of a hidden Markov model given a sequence of observed data.</p> <h2 id="the-hmm-learning-problem">The HMM learning problem</h2> <p>In the HMM learning problem, one learns the hidden transition matrix and conditional probability on states from a sequence of observations.</p> <p>Consider a hidden discrete-time Markov chain with random variables \(X_t\) that take values in a set \(S_X = \{1,\cdots,N\}\) of \(N \in \mathbb{N}\) elements. We assume the time-homogeneous Markov property such that \(\mathbb{P}(X_t|X_{t-1})\) is independent of \(t\), which leads to the definition of the time-independent transition matrix \begin{equation} A = {a_{ij}} = \mathbb{P}(X_t=j|X_{t-1}=i). \end{equation} The initial state distribution is given by \begin{equation} \pi = {\pi_j} = \mathbb{P}(X_1 = j), \quad \sum_{j=1}^N \pi_j = 1. \end{equation} Value of \(X_t\) is not observed directly. Rather, we can observe \(Y_t\) that takes values in a set \(S_Y = \{1,\cdots,M\}\). Moreover, the probability of a certain observation at time \(t\) for state \(j\) is denoted as \begin{equation} b_j(k) = \mathbb{P} (Y_t = k | X_t = j). \end{equation} Taking into account all possible values of \(Y_t\) and \(X_t\), we obtain the \(N \times M\) observation matrix \(B = \{b_j(k)\}\).</p> <p>An observation sequence is given by \(Y = \{Y_1 = y_1\), \(Y_2 = y_2, \cdots, Y_T = y_T\}\). The HMM learning problem is then to estimate the HMM parameters \(A\) and \(B\) based on the observation sequence \(Y\), and the HMM model \(\lambda = (\pi, A, B)\).</p> <h2 id="baum-welch-algorithm">Baum-Welch algorithm</h2> <p>We explain the Baum-Welch algorithm that finds the maximum likelihood estimate of the HMM transition matrix \(A\) and observation matrix \(B\).</p> <h3 id="compute-maximum-likelihood-estimate">Compute maximum likelihood estimate</h3> <p>The maximum likelihood estimate (MLE) of the probability \(a_{ij}\) of a particular transition from state \(i\) to \(j\) is</p> <p>\begin{equation} \hat{a}_{ij} = \frac{\text{expected number of transitions from state } i \text{ to } j}{\text{expected number of transitions from state } i}, \label{eq:MLE_a} \end{equation}</p> <p>and the MLE of the probability \(b_j(k)\) of a given label \(k\) from the observation \(Y\), given a state \(j\), is</p> <p>\begin{equation} \hat{b}_j(k) = \frac{\text{exp number of times in state } j \text{ and observing } k}{\text{exp number of times in state } j}. \label{eq:MLE_b} \end{equation}</p> <p>Therefore, the primary idea of computing \(\hat{a}_{ij}\) and \(\hat{b}_j(k)\) is to count all those “expected” numbers in the equation (\ref{eq:MLE_a}) and (\ref{eq:MLE_b}). The Baum-Welch algorithm iteratively estimates the counts, by initialising with a proper guess of the transition and observation probabilities. The estimated probabilities are then used to derive better and better probabilities in the following iterations.</p> <h3 id="compute-hata_ij-and-hatb_jk">Compute \(\hat{a}_{ij}\) and \(\hat{b}_j(k)\)</h3> <p>Let’s proceed with computing \(\hat{a}_{ij}\). Define the probability \(\xi_t(i,j)\) as the probability of being in state \(i\) at time \(t\) and state \(j\) at time \(t+1\), given the observation sequence and the model \(\lambda\),</p> <p>\begin{equation} \xi_t(i,j) = \mathbb{P} (X_t = i, X_{t+1} = j | Y, \lambda). \end{equation}</p> <p>The expected number of transitions from state \(i\) to \(j\) is then the sum of \(\xi\) over all \(t\). The total expected number of transition from state \(i\) is obtained by summing over all transitions out of state \(i\). Hence,</p> <p>\begin{equation} \hat{a}_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t (i,j)}{\sum_{k=1}^N \sum_{t=1}^{T-1} \xi_t (i,k)}. \label{eq:xi_def} \end{equation}</p> <p>We also need a formula for computing the observation probability \(\hat{b}_j(k)\). Define the probability \(\gamma_t(j)\) as the probability of being in state \(j\) at time \(t\), given the observation sequence and the model \(\lambda\),</p> <p>\begin{equation} \gamma_t(j) = \mathbb{P} (X_t = j | Y, \lambda). \end{equation}</p> <p>This enables us to compute \(\hat{b}_j(k)\). For the numerator, we sum \(\gamma_t(j)\) for all time steps \(t\) in which the observation \(y_t = k\). For the denominator, we sum \(\gamma_t(j)\) over all time steps \(t\). Namely,</p> <p>\begin{equation} \hat{b}_j(k) = \frac{\sum_{y_t = k} \gamma_t(j)}{\sum_{t=1}^{T} \gamma_t(j)}. \label{eq:gamma_def} \end{equation}</p> <h3 id="compute-xi_tij-and-gamma_tj">Compute \(\xi_t(i,j)\) and \(\gamma_t(j)\)</h3> <p>To compute \(\xi\) and \(\gamma\), we need define two more specific probabilities \(\alpha\) and \(\beta\). We define the forward path probability \(\alpha_t(j)\) as the probability of being in state \(j\) after seeing the first \(t\) observations,</p> <p>\begin{equation} \alpha_t(j) = \mathbb{P} (y_1, y_2, \cdots, y_t, X_t = j | \lambda), \end{equation}</p> <p>and, define the backward path probability \(\beta_t (j)\) as the probability of seeing the observations from time \(t+1\) to the end, given that we are in state \(j\) at time \(t\),</p> <p>\begin{equation} \beta_t(j) = \mathbb{P} (y_{t+1}, y_{t+2}, \cdots, y_T | X_t = j, \lambda). \end{equation}</p> <p>By Bayes’ theorem, we can re-write \(\xi\) and \(\gamma\) as</p> <p>\begin{equation} \xi_t(i,j) = \frac{\mathbb{P} (X_t=i,X_{t+1},Y | \lambda)}{\mathbb{P} ( Y | \lambda) }, \quad \gamma_t (j) = \frac{\mathbb{P} (X_t = j, Y | \lambda)}{\mathbb{P} (Y | \lambda)}. \label{eq:xi_gamma_comp} \end{equation}</p> <p>Note that the denominator is the probability of the observation and can be computed in multiple ways,</p> <p>\begin{equation} \mathbb{P} (Y | \lambda) = \sum_{j=1}^N \alpha_T (j) = \sum_{j=1}^N \pi_j \beta_0 (j). \label{eq:ProbObservations} \end{equation}</p> <p>The numerators in (\ref{eq:xi_gamma_comp}) can be expressed in terms of \(\alpha\) and \(\beta\):</p> <p>\begin{equation} \mathbb{P} (X_t=i,X_{t+1},Y | \lambda) = \alpha_t(i) a_{ij} b_j(y_{t+1}) \beta_{t+1} (j), \end{equation} \begin{equation} \mathbb{P} (X_t = j, Y | \lambda) = \alpha_t(j) \beta_t(j). \end{equation}</p> <h3 id="compute-alpha_tj-and-beta_tj">Compute \(\alpha_t(j)\) and \(\beta_t(j)\)</h3> <p>It remains to calculate \(\alpha\) and \(\beta\). From their definitions, we can derive an iteration equation for each. Then we can solve them for all time steps by assigning a starting value. To be specific,</p> \[\alpha_1 (j) = \pi_j b_j(y_1), \text{ } 1 \leq j \leq N,\] <p>\begin{equation} \alpha_t (j) = \sum_{i=1}^N \alpha_{t-1} (i) a_{ij} b_j(y_t), \text{ } 1 \leq j \leq N, 2 \leq t \leq T, \label{eq:ForwardAlgorithm} \end{equation}</p> <p>and,</p> \[\beta_T (j) = 1, \text{ } 1 \leq j \leq N,\] <p>\begin{equation} \beta_t (j) = \sum_{i=1}^N a_{ji} b_i(y_{t+1}) \beta_{t+1}(i), \text{ } 1 \leq j \leq N, 1 \leq t \leq T-1. \label{eq:BackwardAlgorithm} \end{equation}</p> <p>In fact, equation (\ref{eq:ForwardAlgorithm}) is called forward algorithm, and (\ref{eq:BackwardAlgorithm}) is called backward algorithm.</p> <h3 id="summary-of-the-baum-welch-algorithm">Summary of the Baum-Welch Algorithm</h3> <p>Hence, we summarise the algorithm as followings.</p> <blockquote> <p><strong>function</strong> Baum-Welch (observations \(Y = \\{ y_1,\cdots,y_T \\}\), number of hidden states \(N\)):</p> <p>     <strong>initialise</strong> \(A\) and \(B\)</p> <p>     <strong>iterate</strong> until convergence</p> <blockquote> <p>compute \(\alpha_t(j)\) using forward algorithm (\ref{eq:ForwardAlgorithm});</p> <p>compute \(\beta_t(j)\) using backward algorithm (\ref{eq:BackwardAlgorithm});</p> <p>compute \(\xi_t(i,j) = \frac{\alpha_t(i) a_{ij} b_j(y_{t+1})\beta_{t+1}(j)}{\sum_{k=1}^N \alpha_T (k)}\), \(\forall t\) and \(j\);</p> <p>compute \(\gamma_t(j) = \frac{\alpha_t(j) \beta_{t}(j)}{\sum_{k=1}^N \alpha_T (k)}\), \(\forall t\), \(i\) and \(j\);</p> <p>compute \(\hat{a}\_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t (i,j)}{\sum_{k=1}^N \sum_{t=1}^{T-1} \xi_t (i,k)}\), \(\forall i\) and \(j\);</p> <p>compute \(\hat{b}\_j(k) = \frac{\sum_{y_t = k} \gamma_t(j)}{\sum_{t=1}^{T} \gamma_t(j)}\), \(\forall i\) and \(j\);</p> </blockquote> <p><strong>return</strong> \(A\), \(B\)</p> </blockquote> <h2 id="improvments-in-implementation">Improvments in implementation</h2> <p>We have observed some deficiencies of the stated Baum-Welch algorithm during practical implementation. In particular,</p> <ol> <li> <p>Both forward and backward algorithms require computations involving products of probabilities. It is easy to see, for example, \(\alpha_t(j)\) tends to 0 exponentially as \(T\) increases. Therefore, any attempt to implement the formula as given above will inevitably result in underflow.</p> </li> <li> <p>The converged estimates for \(A\) and \(B\) are reasonably believed to be a local rather than the global optimum, so that they are sensitive to the initial guess.</p> </li> </ol> <h3 id="scaling">Scaling</h3> <p>To solve the problem stated in (1), we scale the probabilities calculated in each time step. Instead of calculating \(\alpha_t(j)\) following algorithm (\ref{eq:ForwardAlgorithm}), we compute \(\hat{\alpha}(j)\), in order,</p> \[\tilde{\alpha}_1 (j) = \pi_j b_j(y_1), \text{ } 1 \leq j \leq N,\] \[c_1 = \frac{1}{\sum_{j=1}^N \tilde{\alpha}_1(j)},\] \[\hat{\alpha}\_1 (j) = c_1 \tilde{\alpha}_t (j), \text{ } 1 \leq j \leq N,\] \[\tilde{\alpha}\_t (j) = \sum_{i=1}^N \hat{\alpha}\_{t-1} (i) a_{ij} b_j(y_t), \text{ } 1 \leq j \leq N, 2 \leq t \leq T,\] \[c_t = \frac{1}{\sum_{j=1}^N \tilde{\alpha}_t(j)},\] <p>\begin{equation} \hat{\alpha}_t (j) = c_t \tilde{\alpha}_t (j), \text{ } 1 \leq j \leq N. \label{eq:ForwardAlgorithmScale} \end{equation}</p> <p>To make sure the above-mentioned scaling on \(\alpha\) do not distort the probabilities computed in the following such as \(\xi\) and \(\gamma\), the same scaling are applied to \(\beta\) accordingly. Therefore, instead of computing \(\beta_t(j)\) using algorithm (\ref{eq:BackwardAlgorithm}), we compute \(\hat{\beta}_t (j)\), in order,</p> \[\hat{\beta}_T (j) = c_T, \text{ } 1 \leq j \leq N,\] \[\tilde{\beta}\_t (j) = \sum_{i=1}^N a_{ji} b_i(y_{t+1}) \hat{\beta}_{t+1}(i), \text{ } 1 \leq j \leq N, 1 \leq t \leq T-1,\] <p>\begin{equation} \hat{\beta}_t (j) = c_t \tilde{\beta}_t (j), \text{ } 1 \leq j \leq N. \label{eq:BackwardAlgorithmScale} \end{equation}</p> <h3 id="optimal-initialization">Optimal initialization</h3> <p>The converged estimates for \(A\) and \(B\) are usually local optimum, subject to the choice of initial guess of the model \(\lambda^{(0)} = (\pi^{(0)}, A^{(0)}, B^{(0)})\). To attempt to approach the global optimum, we randomly select a sequence of initial guesses and pick the one that results in the largest probability \(\mathbb{P} (Y \\| \lambda)\), the probability of observing the given sequence.</p> <p>Due to the scaling in the revisited forward algorithm (\ref{eq:BackwardAlgorithmScale}), we use formula (\ref{eq:ProbObservations}) and obtain</p> <p>\begin{equation} \mathbb{P} (Y | \lambda) = \frac{1}{\prod_{t=1}^T c_t}. \end{equation}</p> <p>To avoid underflow, we instead compute</p> <p>\begin{equation} \log[ \mathbb{P} (Y | \lambda) ] = - \sum_{t=1}^T \log c_t. \end{equation}</p> <p>We don’t allow completely randomness of the model, which might largely increase the computational efforts required for the maximisation. We fix</p> <p>\begin{equation} \pi^{(0)}_j = \frac{1}{N}, \text{ } b_j(k) = \frac{1}{M}, \text{ for }\forall j \text{ and } k, \end{equation}</p> <p>and only allow \(a_{ij}\) to be uniformly distributed with respect to \(\sum_j a_{ij} = 1\) for \(\forall i\).</p>]]></content><author><name>Victor Wang</name></author><category term="maths"/><category term="maths"/><summary type="html"><![CDATA[The Hidden Markov Model (HMM) assumes the system being modelled to be Markov process with unobserved (i.e. hidden) states. In particular, one problem is to learn the hidden Markov transition matrix and conditional probability on Markov states from a sequence of observations. We explain the Baum-Welch algorithm which finds the maximum likelihood estimate of these parameters of a hidden Markov model given a sequence of observed data.]]></summary></entry><entry><title type="html">Solving a linear system</title><link href="https://vicaws.github.io/blog/2017/solving-linear-system/" rel="alternate" type="text/html" title="Solving a linear system"/><published>2017-11-28T15:12:00+00:00</published><updated>2017-11-28T15:12:00+00:00</updated><id>https://vicaws.github.io/blog/2017/solving-linear-system</id><content type="html" xml:base="https://vicaws.github.io/blog/2017/solving-linear-system/"><![CDATA[<p>To solve a linear system \(Ax=b\), we can either invert the coefficient matrix \(A\) and multiply \(b\), or numerically approximate the roots by iterative methods. In the first approach, matrix inversion is always a pain, which we possibly mitigate by matrix factorizations on \(A\). The concern of the second approach is the convergence of the numerical scheme, which depends on the properties of \(A\).</p> <p>In this post, we study multiple algorithms for QR factorizations, SVD decomposition and iterative root solving. We implement the algorithms in MATLAB and compare their performance, along with the MATLAB performance of in-built functions.</p> <h2 id="qr-factorization">QR factorization</h2> <p>An \(m \times n\) matrix \(A\) can be factorized as \begin{equation} A = QR, \end{equation} with \(Q\) unitary (such that \(Q^*Q = QQ^* = I\)) and \(R\) upper triangular. Given \(Q\) and \(R\), we solve \(Ax=b\) by multiplying by \(Q^*\) and back-solving \(Rx = Q^*b\).</p> <p>We implement three algorithms and contrast their performances. Table 1 lists the three QR algorithms we study and the MATLAB in-built <code class="language-plaintext highlighter-rouge">qr()</code> function that returns \(Q\) and \(R\) given an \(A\). It is computationally much easier of inverting \(R\) than directly inverting \(A\).</p> <table> <thead> <tr> <th><strong>Algorithm</strong></th> <th style="text-align: right"><strong>flops</strong></th> </tr> </thead> <tbody> <tr> <td>Modified Gram-Schmidt</td> <td style="text-align: right">\(2mn^2\)</td> </tr> <tr> <td>Householder reflections</td> <td style="text-align: right">\(2mn^2 - \frac{2}{3} n^3\)</td> </tr> <tr> <td>Givens rotations</td> <td style="text-align: right">\(3mn^2 - n^3\)</td> </tr> <tr> <td>MATLAB <code class="language-plaintext highlighter-rouge">qr()</code></td> <td style="text-align: right">\(3mn^2 - n^3\)</td> </tr> </tbody> </table> <p><strong>Table 1</strong> - List of QR algorithms.</p> <p>We simulate a matrix \(A\) with exponentially-decayed singular values in the following ways:</p> <ol> <li>Simulate an \(m \times m\) matrix \(U\) and \(n \times n\) matrix \(V\) from independent standard Gaussian distribution.</li> <li>Define \(r = \min(m,n)\), construct an \(r \times r\) diagonal matrix \(D\) where \(D_{11} = 1/c\) and \(D_{(i+1)(i+1)} = D_{ii}/c\) for all \(2 \leq i \leq r\).</li> <li>Compute \(A = U(:,1:r) D V(:,1:r)^\top\).</li> </ol> <p>By construction, the ordered diagonal element of the decomposed upper triangular matrix \(R\) should decay exponentially. Figure 1 displays the ordered diagonal elements of \(R\) computed from the four algorithms. In this instance, \(m=n=80\) and \(c=2\). Their values shown in \(y\)-axis is on log-scale for clearer illustration. We do observe the four algorithms highly agree with each other up to the 58th largest value. Afterwards, there are slight discrepancies, but at trivial level \(\exp(-40)\). Those differences shall be owe to machine error. Overall, the ordered diagonal elements of \(R\) do decay exponentially, as expected.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D1_DiagRComparision.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 1</strong> - Diagonal elements of \(R\) by various algorithms.</p> <h2 id="iterative-methods">Iterative methods</h2> <p>To avoid matrix inversion, iterative methods are used to solve linear system numerically within specific tolerant errors. Table 2 lists the iterative methods we will cover in the followings. A brief description of the iteration form is shown in the table.</p> <table> <thead> <tr> <th>Algorithm</th> <th>Iterative Form</th> </tr> </thead> <tbody> <tr> <td>Jacobi</td> <td>\(x^{(k+1)} = T_J x^{(k)} + c_J\)</td> </tr> <tr> <td>Orthomin(1)</td> <td>\(x^{(k+1)} = x^{(k)} + \alpha_k r^{(k)}\), \(r^{(k)} = b - A x^{(k)}\), \(\min || r^{(k+1)} ||_2\) along \(A r^{(k)}\)</td> </tr> <tr> <td>Steepest Descent</td> <td>\(x^{(k+1)} = x^{(k)} + \alpha_k r^{(k)}\), \(r^{(k)} = b - A x^{(k)}\), \(\min|| x - x^{(k+1)} ||_A\) along \(r^{(k)}\)</td> </tr> <tr> <td>Orthomin(j)</td> <td>\(x^{(k+1)} = x^{(k)} + \alpha_k p^{(k)}\), \(p^{(k)} = r^{(k)} - \sum_{q=k+1-j}^{k-1} \beta_q p^{(q)}\), \(r^{(k)} = b - A x^{(k)}\), \(\min || r^{(k+1)} ||_2\), \(A p^{(k)} \perp A p^{(q)}\), \(k+1-j \leq q &lt; k\)</td> </tr> <tr> <td>Conjugate Gradient</td> <td>\(x^{(k+1)} = x^{(k)} + \alpha_k p^{(k)}\), \(p^{(k)} = r^{(k)} - \beta_{k-1} p^{(k-1)}\), \(r^{(k)} = b - A x^{(k)}\), \(\min || x- x^{(k+1)} ||_A\), \(r^{(k)} \perp r^{(q)}\), \(p^{(k)} \perp A p^{(q)}\), \(q &lt; k\)</td> </tr> <tr> <td>GMRES</td> <td>\(x^{(k)} = Q_k y^{(k)}\), \(y^{(k)} = \text{arg}\min_y ||b-AQ_ky||\), \(\min || r^{(k)} ||_2\) over \(b+\mathcal{K}_k(b,A)\)</td> </tr> </tbody> </table> <p><strong>Table 2</strong> - List of Iterative Methods</p> <h3 id="jacobi-t">Jacobi \(T\)</h3> <p>The Jacobi algorithm defines a matrix \(T_J\) and a vector \(c_J\), \begin{equation} T_J = I - D^{-1} A, \quad c_J = D^{-1} b, \end{equation} such that \(x=T_Jx+c_J\) is equivalent to \(Ax = b\). Note here \(D\) is the diagonal matrix of \(A\). Defining error as \(e^{(n)} = x - x(n)\) gives the formula \(e^{(n)} = T^n e^{(0)}\). Therefore, the convergence of the Jacobi algorithm hinges on understanding the properties of \(T\) so that \(\|T^n e^{(0)}\|\) converges to zero.</p> <p>A theorem states that \(\|T^n\|\) converges to zero if and only if the spectral radius \(\rho(T) &lt; 1\). In fact, this theorem is built on top of the fact that \begin{equation} \lim_{n \rightarrow \infty} ||T^n||^{\frac{1}{n}} = \rho(T), \label{eq:TConverge} \end{equation} in which case \(\|T^n\| = \rho^n(T) \rightarrow \infty\) as \(n \rightarrow \infty\) if \(\rho(T)&lt;1\). To numerically demonstrate the property of \(T\) in \eqref{eq:TConverge}, we simulate a \(100\times 100\) matrix \(T\) from independent standard Gaussian distribution, and then calculate the ratio \(\|T^n\|^{1/n} / \rho(T)\) for different \(n\). Moreover, we try matrix norm defined in \(l^1\), \(l^2\) and \(l^{\infty}\).</p> <p>In Figure 2, we plot the ratio defined under \(l^1\), \(l^2\) and \(l^\infty\)-norm for \(n = 2^j\) where \(j=1,2,\cdots8\). As \(n\) increases, all ratios decay to 1 almost around the same value of \(n\), though starting from different points. \(l^1\) and \(l^{\infty}\)-norm result in almost the same ratio along \(n\). This is expected from the definition of various matrix norms.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D2_TRatio.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 2</strong> - Convergence of \(\|T^n\|^{1/n}\) to \(\rho(T)\).</p> <h3 id="convergence-performance-of-jacobi-orthomin1-and-steepest-descent">Convergence performance of Jacobi, Orthomin(1) and Steepest Descent</h3> <p>Next we compare the rate of convergence of algorithm Jacobi, Orthomin(1) and Steepest Descent. The residual \(\|r^{(k)}\|\) at \(k\)-th iteration is defined as \(\|b-Ax^{(k)}\|\). In particular, we construct a \(1000\times 1000\) strictly row diagonal dominant (SRDD) matrix \(A\), and solve \(Ax=b\) (for some \(b\)) iteratively. The termination criteria is either the residual hits the tolerant error <code class="language-plaintext highlighter-rouge">tol = 1e-8</code> or the the number of iterations hits the maximum <code class="language-plaintext highlighter-rouge">max_iter = 1e6</code>. Figure 3 shows how residual reduces over iteration. Note that all algorithms converge linear in log scale. Both Jacobi and Orthomin(1) converge at 6-th iteration, while Steepest Descent converges 1 iteration further. We further show how convergence over computational time in Figure 4. It seems to imply that the Steepest Descent algorithm not only takes more iterations, but also more time. It is not true in general.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D2_ConvergeIter.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 3</strong> - Log-scale residual over iterations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D2_ConvergeTime.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 4</strong> - Log-scale residual over time.</p> <p>Recall the convergence theorem for the Jacobi algorithm, which says that if \(A\) is SRDD, then the algorithm converges to the solution of the linear system of equations \(A^{-1}b\) at a linear rate \(\|x^{(k)}-A^{-1}b\| \leq \mathcal{O}(\gamma^{k})\) with \(\gamma \leq \rho(T_J) &lt; 1\). This is consistent with the observations in Figure 4.</p> <p>Finally, we scale the problem by adjusting size of matrix \(A\) and see how the convergence time behaves for all the three algorithms along with the MATLAB in-built \(LU\) factorization function. Figure 5 displays the time of convergence with \(m\), the size of matrix. There is no surprise that the required time of convergence increases with matrix size, for all algorithms. However, the MATLAB in-built function is more sensitive to change in matrix size. In particular, when \(m\) exceeds around 120, the MATLAB function requires more and more time than the other 3 algorithms do for convergence. In addition, Jacobi gives worse convergence time than the other 2 as \(m\) scales up.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D2_JOSL.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 5</strong> - Time of convergence.</p> <h3 id="convergence-performance-of-orthominj-and-conjugate-gradient">Convergence performance of Orthomin(j) and Conjugate Gradient</h3> <p>The Orthomin(j) and the Conjugate Gradient algorithm can be viewed as an augmented version for Orthomin(1) and Steepest Descent, respectively, by optimizing the search direction to accelerate convergence.</p> <p>First we take a close look at how \(j\) affects the convergence performance of the Orthomin(j) algorithm. We generate a symmetric \(1000 \times 1000\) matrix \(A\). Its diagonal forms a linear span from 1 to 2, and all the off-diagonal entries follow the independent standard Gaussian distribution. The termination criteria of iterative solving \(Ax=b\) is either the residual hits the tolerant error <code class="language-plaintext highlighter-rouge">tol = 1e-8</code> or the the number of iterations hits the maximum <code class="language-plaintext highlighter-rouge">max_iter = 1e6</code>.</p> <p>Figure 6 shows, at convergence, the number of iterations and computational time required for different \(j\). It shows strong evidence that the convergence rate (either measured in number of iterations or time) for Orthomin(j) is indifferent for any \(j \geq 2\). Nevertheless, Orthomim(1) performs significantly worse due to the lack of optimal search direction. Therefore, in the following analysis, we will primarily use Orthomin(2) algorithm in the comparison with other algorithms, without worrying too much about larger \(j\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D3_VaringJ.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 6</strong> - Different \(j\) - convergence of Orthomin(j).</p> <p>We contrast the Conjugate Gradient algorithm with Orthomin(2) and Orthomin(8) in in terms of convergence rate, as shown in Figure 7. As illustrated before, there shall be no difference between Orthomin(2) and Orthomin(8). In addition, we observe that the Conjugate Gradient algorithm has almost the same convergence as the other 2.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D3_CGO_Iter.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 7</strong> - Log-scale residual over iterations.</p> <p>Finally, we scale the matrix size and see how convergence time changes for all algorithms. In addition, we also include MATLAB in-built \(LU\) factorization function into comparison. Figure 8 plots the time of convergence with \(m\), the size of matrix. There is no surprise that the required time of convergence increases with matrix size, for all algorithms. However, the MATLAB in-built function is more sensitive to change in matrix size. In particular, when \(m\) exceeds around 250, the MATLAB function requires more and more time than the other 2 algorithms do for convergence. Again, the Conjugate Gradient algorithm has almost the same convergence as the Orthomin(2), which is consistent with observations in Figure 7.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D3_CGOL.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 8</strong> - Log-scale residual over iterations.</p> <p>Note that above analysis is based on the \(1000 \times 1000\) matrix \(A\) whose eigenvalues form a linear span from 1 to 2. The eigenvalues of \(A\) has very narrow range and are distributed uniformly. To see how the convergence can be impacted by the distributions of \(A\)’s eigenvalues, we regenerate \(A\) with differently distributed eigenvalues. In particular, we generate 4 \(A\) with eigenvalues that</p> <ul> <li>have uniform narrow range;</li> <li>have uniform wide range;</li> <li>have very large and small value;</li> <li>have repeated value.</li> </ul> <p>Next we apply the Conjugate Gradient algorithm to solve \(Ax=b\), and obtain the convergence rate shown in Figure 9. First of all, if \(A\)’s eigenvalues contain very large eigenvalues (e.g. the largest one is \(10^8\) times of the smallest), then the algorithm does not seem to converge at all (note in this example we allow at most \(10^6\) iterations). Narrow-ranged eigenvalues result in faster convergence than wide-ranged eigenvalues do. Repeated eigenvalues also tend to slow down the convergence.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D3_Eignvalues.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 9</strong> - Impact of distributions of eigenvalues.</p> <h3 id="convergence-performance-of-gmres">Convergence performance of GMRES</h3> <p>The last iterative method we implement is the GMRES algorithm. In general, we consider GMRES as the last resort for solving linear system, since it does not require extra conditions on \(A\) but can be quite complex.</p> <p>First, we analyse how the distribution of \(A\)’s eigenvalues influences the convergence of the algorithm. Consider a \(100\times 100\) symmetric matrix \(A\) where its eigenvalues are uniformly distributed within the range \([-c,c]\).</p> <p>In Figure 10, we show the log-scale residual over iterations for the same GMRES algorithm given different \(A\), that is characterized by \(c\). Larger \(c\) implies that \(A\) has larger eigenvalues as well as a wider range of eigenvalues. Respectively, the convergence is expected to be slower for \(A\) with larger \(c\). This is confirmed by Figure 10.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D4_GMRES_VaryingC_Iter.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 10</strong> - Impact of distributions of eigenvalues.</p> <p>We also compare the convergence performance of the GMRES with other algorithms for solving the same linear system. In particular, Figure 11 displays the time of convergence with \(m\), the size of the matrix. Still, the required time of convergence increases with matrix size. All the other 3 algorithms seem to outperform GMRES when \(m&lt;200\). For matrix size \(m&gt;200\), the MATLAB built in LU factorization is beaten by the others, and its associated time of convergence seem to grow at least quadratically. However, all the other algorithms only require linearly growing time for convergence as matrix size scale up. Overall, the GMRES algorithm always converges slower than the Orthomin(2) and Conjugate Gradient does.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D4_GMRES_LU_O_CG.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 11</strong> - Time of convergence.</p> <h2 id="svd">SVD</h2> <p>In the section, we discuss SVD, the singular value decomposition, of the \(m\times n\) matrix \(A\).</p> <p>We have implemented a two-step algorithms to decompose the matrix \(A\),</p> <ol> <li>Apply the Golub-Kahan bi-diagonalization to \(A\) and obtain an upper bi-diagonal matrix \(B\), as well unitary matrix \(U\) and \(V\).</li> <li>Apply the Cholesky iteration (dqds) to B and get the singular values.</li> </ol> <p>Figure 12 shows a comparison between the results from our implementation <code class="language-plaintext highlighter-rouge">dqds()</code> and the MATLAB in-built <code class="language-plaintext highlighter-rouge">svd()</code> function. The singular values are ordered from the largest to the smallest from left to right. Visually the difference is small.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/solving-linear-system/D5_SVD.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 12</strong> - Singular values comparison.</p> <p>See below the MATLAB script of the <code class="language-plaintext highlighter-rouge">dqds()</code> function:</p> <d-code block="" language="javascript"> function [sigma] = dqds(B, tol, maxiter) [m, n] = size(B); [A, C] = spdiags(B); NofDiags = min(m,n); q = A * (C==0); q = q.^2; e = A * (C==1); e = e(1:end-1).^2; for i= 1:maxiter qnew = zeros(NofDiags, 1); enew = zeros(NofDiags-1, 1); d = q(1); for k = 1:NofDiags-1 qnew(k) = d + e(k); enew(k) = e(k) * q(k+1) / qnew(k); d = d * q(k+1) / qnew(k); end qnew(NofDiags) = d; if abs(e(end)) &lt; tol sigma = sqrt(qnew); sigma = sort(sigma, 'descend'); disp('converges after'); disp(i); disp('iterations. '); break end q = qnew; e = enew; end end </d-code>]]></content><author><name>Victor Wang</name></author><category term="maths"/><category term="maths"/><summary type="html"><![CDATA[To solve a linear system \(Ax=b\), we can either invert the coefficient matrix \(A\) and multiply \(b\), or numerically approximate the roots by iterative methods. In the first approach, matrix inversion is always a pain, which we possibly mitigate by matrix factorizations on \(A\). The concern of the second approach is the convergence of the numerical scheme, which depends on the properties of \(A\).]]></summary></entry><entry><title type="html">Ribbon bridge</title><link href="https://vicaws.github.io/blog/2017/ribbon-bridge/" rel="alternate" type="text/html" title="Ribbon bridge"/><published>2017-11-19T15:12:00+00:00</published><updated>2017-11-19T15:12:00+00:00</updated><id>https://vicaws.github.io/blog/2017/ribbon-bridge</id><content type="html" xml:base="https://vicaws.github.io/blog/2017/ribbon-bridge/"><![CDATA[<p>A ribbon bridge hangs between pillars on either side of a river. We would like to model the shape of the bridge by deriving a governing differential equation from the energy minimisation approach. Thereafter we solve the differential equation and arrive at a formula for the shape.</p> <h2 id="modelling-the-shape">Modelling the shape</h2> <p>We model the shape of a ribbon bridge in a 2-dimensional \((x,y)\) plane. Denote the height of the bridge at horizontal position \(x\) as \(y(x)\), and the bridge spans from \(x_a\) to \(x_b\). Therefore, we have the gravitational potential energy \begin{equation} E_\text{G} = \int_{x_a}^{x_b} \rho g y \sqrt{1 + (y’)^2} \mathrm{d} x, \label{eq:GravPotentialEnergy} \end{equation} where \(\rho\) is the mass density (mass per unit length) of the bridge and \(g\) is the gravitational constant. By using the fact that the length of the bridge is a fixed constant \(L\), we formulate \begin{equation} \int_{x_a}^{x_b} \sqrt{1 + (y’)^2} \mathrm{d} x = L. \label{eq:ConstantLength} \end{equation}</p> <p>The idea is then to find a formulation for \(y\) such that the energy of the system, \(E_\text{G}\) in (\ref{eq:GravPotentialEnergy}), is minimised with the constraint specified as (\ref{eq:ConstantLength}). This is equivalent to \begin{equation} \min_{y} \int_{x_a}^{x_b} f(y,y’) - \frac{\lambda L}{x_b - x_a} \mathrm{d} x, \label{eq:MinimisationProblem} \end{equation} where \(\lambda\) is the Lagrangian multiplier, and \begin{equation} f(y, y’) = ( \rho g y + \lambda ) \sqrt{1 + (y’)^2}. \end{equation}</p> <p>The Beltrami Identity states that the minimal is reached when (\ref{eq:BeltramiIdentity}) holds for some constant \(c\), \begin{equation} f - y’ \frac{\partial f}{\partial y’} = c, \label{eq:BeltramiIdentity} \end{equation} which leads to \begin{equation} \rho g y + \lambda = c \sqrt{1 + (y’)^2}. \label{eq:GoverningODE} \end{equation}</p> <p>We take square on both sides of the equation (\ref{eq:GoverningODE}) and then take first derivative against \(x\) on both sides, to obtain \begin{equation} y’’ - \left( \frac{\rho g}{c} \right)^2 y = \frac{\rho g \lambda}{c^2}. \label{eq:GoverningODE1} \end{equation} Note that we omit the physically-unrealistic solution \(y'=0\) in the derivation to (\ref{eq:GoverningODE1}). The complete solution \(y\) consists of a complementary solution \(y_c\) and a particular solution \(y_p\), namely \(y = y_c + y_p\). \(y_c\) is the solution to the corresponding homogeneous problem and have the form \begin{equation} y_c = A \cosh \left( \frac{\rho g}{c} (x-x_0) \right), \end{equation} where the constants \(A\) and \(x_0\) remain to be determined. Substituting \(y_c\) into (\ref{eq:GoverningODE}) yields \(A = c / \rho g\). The particular solution shall have a form \(y_p = B\) for a constant \(B\). Plugging \(y_p\) into (\ref{eq:GoverningODE1}) gives \(B = - \lambda / \rho g\). Hence, \begin{equation} y = \frac{c}{\rho g} \cosh \left( \frac{\rho g}{c} (x - x_0) \right) - \frac{\lambda}{\rho g} \label{eq:ShapeSolution} \end{equation} is the solution to the bridge shape governing equation (\ref{eq:GoverningODE}) with three constants \(c\), \(x_0\) and \(\lambda\) to be determined by boundary conditions.</p> <h2 id="shape-calibration">Shape calibration</h2> <p>We calibrate the shape solution (\ref{eq:ShapeSolution}) to a real world stress ribbon bridge by choosing parameters \(c\), \(x_0\) and \(\lambda\).</p> <p>We look at the lignon-Loex Bridge, that crosses the river Rhone, and is situated in the suburbs of Geneva. It is formed by a stress ribbon of one span of length \(S=136\text{m}\); the sag at mid-span is \(D=5.6\text{m}\). We assume the two ends of the bridge are at the same horizon, and put the bridge in the (\(x\),\(y\)) coordinate such that \(x_a = 0\) and \(x_b = S\). Therefore, we impose three boundary conditions to (\ref{eq:ShapeSolution}), \begin{equation} y(x_a) = 0, \quad y(x_b) = 0, \quad y(\frac{S}{2}) = -D. \label{eq:Boundary} \end{equation} Denoting \(c^* = c/ \rho g\), \(\lambda^* = \lambda / \rho g\), we plug the boundary conditions \ref{eq:Boundary} into \ref{eq:ShapeSolution} and get numerical results. \begin{equation} c* \approx 413.79 \mathrm{m}^{-1}, \quad x_0 \approx 68 \mathrm{m}, \quad \lambda^* \approx 419.39 \mathrm{m}^{-1}. \end{equation}</p> <p>The gravitational constant \(g\) is approximately \(9.8 \text{m}/\text{s}^2\), and the density of the bridge (made of concrete) is around \(2400 \mathrm{kg}/\mathrm{m}^3\). Assuming the bridge has an average width of \(2\)m and thickness of \(0.5\)m, then the density per unit length is \(\rho = 2400 \mathrm{kg}/\mathrm{m}\). Therefore, \begin{equation} c \approx 973227 \mathrm{N}, \quad \lambda \approx 986399 \mathrm{N}. \end{equation}</p> <p>Moreover, we show the plot of the calibrated shape in Figure 1.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/ribbon-bridge/CalibratedShape.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 1</strong> - Calibrated shape.</p> <p>\(x_0\) is the midpoint of \(x_a\) and \(x_b\), the two endpoints of the bridge. The value of \(c\) seems differ little from the value of \(\lambda\). In fact, by initialising the coordinate properly, we can make \(c = \lambda\). For example, let \begin{equation} y(x_a=-\frac{S}{2} + x_0) = D, \quad y(x_b=\frac{S}{2} + x_0) = D, \quad y(x_0) = 0, \end{equation} we can show \(c = \lambda\).</p> <p>The unit of \(c\) and \(\lambda\) is Newton, by matching the dimension in the governing differential equation. Indeed, the Lagrangian multiplier \(\lambda\) represents a meaningful physical quantity in this scenario. We illustrate in the next section.</p> <h2 id="interpreting-lambda">Interpreting lambda</h2> <p>We know from dimensional analysis that \(\lambda\) represents a force. The next question is, what force exactly?</p> <p>To demonstrate this, we consider the bridge shape in a slightly different situation. Now the bridge is pinned at one end, but at the other end the bridge is free to slide along the top of the pillar (i.e. the total bridge length between the pillars can change). We apply a constant horizontal force \(T\) to this end of the bridge. In this situation, the total energy of the system is composed of the gravitational potential energy (\ref{eq:GravPotentialEnergy}) and the work done by the horizontal force \(T\), which is</p> <p>\begin{equation} W = T \cdot \Delta L = T(L - \int_{x_a}^{x_b} \sqrt{1+(y’)^2} \mathrm{d}x). \end{equation} and therefore the total energy is \begin{equation} E_\text{Tot} = E_\text{G} - W. \end{equation} Hence, we are seeking for \(y(x)\) that minimises the total energy \(E_\text{Tot}\), namely, \begin{equation} \min_{y} \int_{x_a}^{x_b} (\rho g y + T) \sqrt{1+(y’)^2} - \frac{TL}{x_b - x_a} \mathrm{d} x. \label{eq:Minimisation2} \end{equation}</p> <p>Comparing (\ref{eq:Minimisation2}) with the minimisation problem (\ref{eq:MinimisationProblem}) defined in the previous scenario, they become identical if we let \begin{equation} \lambda = T. \end{equation}</p> <p>Hence, it is reasonably to interpret the Lagrangian multiplier \(\lambda\) as the tension force transmitted through the bridge.</p> <h2 id="conclusion">Conclusion</h2> <p>We derive the shape of a hanging ribbon bridge by minimising the total energy of the bridge system subject to constraints. To solve the constrained minimisation problem, we have used Lagrangian multiplier, which can be interpreted as tension force transmitted through the bridge. We also calibrate the shape solution to a real-world stress ribbon bridge, and estimate a typical magnitude for the tension force.</p>]]></content><author><name>Victor Wang</name></author><category term="maths"/><category term="maths,"/><category term="physics"/><summary type="html"><![CDATA[A ribbon bridge hangs between pillars on either side of a river. We would like to model the shape of the bridge by deriving a governing differential equation from the energy minimisation approach. Thereafter we solve the differential equation and arrive at a formula for the shape.]]></summary></entry><entry><title type="html">Hanging catenary</title><link href="https://vicaws.github.io/blog/2017/hanging-catenary/" rel="alternate" type="text/html" title="Hanging catenary"/><published>2017-11-19T12:12:00+00:00</published><updated>2017-11-19T12:12:00+00:00</updated><id>https://vicaws.github.io/blog/2017/hanging-catenary</id><content type="html" xml:base="https://vicaws.github.io/blog/2017/hanging-catenary/"><![CDATA[<p>A catenary is the curve that an idealised hanging chain or cable assumes under its own weight when supported only at its ends. For example, a stressed ribbon bridge hanging between pillars on either side of a river forms a shape of catenary. The catenary shape can be derived by minimising the total energy of the hanging chain. We would like to formulate a discrete energy objective and minimise it numerically.</p> <h2 id="modelling-the-shape">Modelling the shape</h2> <p>We model a catenary of \(n+1\) beams in a 2-dimensional \((x,y)\) plane, with the first beam fixed at the origin and the final beam fixed at a fraction \(\gamma \in (0,1)\) of the total length of all the beams. Suppose each beam is of length \(L\) and of mass \(m\). Let \((x_i, y_i)\), \(i=1,\cdots,n+1\) be the coordinates of the end of the beams and \((x_0,y_0) = (0,0)\) the coordinate of the start point of the first beam. Assuming that the gravity acts at the middle of each beam, then we have the gravitational potential energy</p> <p>\begin{equation} E_\text{G} = \sum_{i=1}^{n} mg \cdot \frac{1}{2}(y_{i-1} + y_{i}), \end{equation}</p> <p>where \(g\) is the gravitational constant. Hence, the resulting problem minimises the energy of the system \(E_\text{G}\). Since \(mg\) is constant, we define the problem as</p> <p>\begin{equation} \min_{x,y} \quad \frac{1}{2} y_0 + y_1 + \cdots + y_n + \frac{1}{2} y_{n+1} \label{eq:MinProblem} \end{equation}</p> \[\text{subject to} \quad (x_0, y_0) = (0,0),\] \[(x_{n+1}, y_{n+1}) = (\gamma (n+1)L, Y),\] \[(x_i - x_{i+1})^2 + (y_i - y_{i+1})^2 = L^2, \forall i = 0,\cdots, n.\] <p>Here \(Y\) is a constant indicating the vertical position of the end of the final beam.</p> <h2 id="numerical-optimisation">Numerical optimisation</h2> <p>We solve the constrained minimisation problem (\ref{eq:MinProblem}) numerically given values of parameters \(n\), \(\gamma\), \(L\), \(Y\). To be specific, we use <code class="language-plaintext highlighter-rouge">MATLAB Optimization Toolbox</code> to implement the <code class="language-plaintext highlighter-rouge">interior-point</code> algorithm. It is of our interest to study the optimisation performance under different configurations.</p> <p>We define the optimisation configuration matrix in Table 1.</p> <table> <thead> <tr> <th><strong>Parameter</strong></th> <th><strong>Description</strong></th> <th><strong>Default</strong></th> </tr> </thead> <tbody> <tr> <td>\(T_\text{1st}\)</td> <td>Termination tolerance on the first-order optimality</td> <td><code class="language-plaintext highlighter-rouge">1e-6</code></td> </tr> <tr> <td>\(T_\text{C}\)</td> <td>Termination tolerance on the constraint violation</td> <td><code class="language-plaintext highlighter-rouge">1e-6</code></td> </tr> <tr> <td>\((\mathbf{x}^0, \mathbf{y}^0)\)</td> <td>Starting points</td> <td><code class="language-plaintext highlighter-rouge">(0,0)</code></td> </tr> <tr> <td>\(S_\text{G}\)</td> <td>Supplying analytical gradients</td> <td><code class="language-plaintext highlighter-rouge">false</code></td> </tr> <tr> <td>\(S_\text{GH}\)</td> <td>Supplying analytical gradients and Hessian</td> <td><code class="language-plaintext highlighter-rouge">false</code></td> </tr> </tbody> </table> <p><strong>Table 1</strong> - Optimisation configuration matrix.</p> <p>While the default values are given in the last column, we are about to investigate how the optimisation performs by adjusting these parameters. Note that we assume very small tolerant step size and very large tolerant number of iterations and function evaluations, so that the numerical process will only terminate by satisfying the pre-defined tolerances (i.e. \(T_\text{1st}\) and \(T_\text{C}\)). In fact, we take the number of iterations and function evaluations as our key measures of the algorithm efficiency.</p> <p>Table 2 lists the interested measures of optimisation performance. Given fixed stopping criteria, a more efficient process will terminate with less iterations (\(N_\text{I}\)) and function evaluations (\(N_\text{F}\)), while a more effective process will end up will smaller first-order derivative (\(E_\text{1st}\)) and constraint violations (\(E_\text{C}\)).</p> <table> <thead> <tr> <th><strong>Parameter</strong></th> <th><strong>Description</strong></th> </tr> </thead> <tbody> <tr> <td>\(N_\text{I}\)</td> <td>Number of iterations</td> </tr> <tr> <td>\(N_\text{F}\)</td> <td>Number of function evaluations</td> </tr> <tr> <td>\(E_\text{1st}\)</td> <td>Measure of the realised first-order optimality</td> </tr> <tr> <td>\(E_\text{C}\)</td> <td>Maximum of constraint function violations</td> </tr> </tbody> </table> <p><strong>Table 2</strong> - Optimisation performance matrix.</p> <p>To proceed with the numerical process, we assign instance values to the model parameters as listed in Table 3. We will modify some default values to study corresponding impact on the optimisation performance.</p> <table> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td><strong>Parameter</strong></td> <td>\(n\)</td> <td>\(\gamma\)</td> <td>\(L\)</td> <td>\(Y\)</td> </tr> <tr> <td><strong>Value</strong></td> <td>19</td> <td>0.8</td> <td>1</td> <td>5</td> </tr> </tbody> </table> <p><strong>Table 3</strong> - Default values of the model parameters.</p> <h3 id="optimisation-with-default-configurations">Optimisation with default configurations</h3> <p>We start from looking at the results with default configs. Figure 1 displays the resulted shape, and Figure 2 traces the evolution of function values and first-order optimality over iterations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/hanging-catenary/DefaultCatenary.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 1</strong> - Catenary shape.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/hanging-catenary/DefaultOptimPerform.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 2</strong> - Optimisation performance.</p> <h3 id="supplying-gradients-and-hessian">Supplying gradients and Hessian</h3> <p>The default configs do not admit gradients or Hessian, in which case the solvers estimate them via finite differences. The estimation is time-consuming and can be less accurate for higher order of derivatives. In the catenary problem, it is possible to derive analytical form of gradients and Hessian for both objectives and constraints. We monitor changes in optimisation performance by including gradients (\(S_\text{G} = \text{true}\)) and Hessian (\(S_\text{GH} = \text{true}\)) in sequence.</p> <table> <thead> <tr> <th> </th> <th>\(n=19\)</th> <th> </th> <th>\(n=49\)</th> <th> </th> </tr> </thead> <tbody> <tr> <td><strong>Solver</strong></td> <td>\(N_\text{I}\)</td> <td>\(N_\text{F}\)</td> <td>\(N_\text{I}\)</td> <td>\(N_\text{F}\)</td> </tr> <tr> <td><strong>Default</strong></td> <td>51</td> <td>2299</td> <td>565</td> <td>61639</td> </tr> <tr> <td>\(S_\text{G} = \text{true}\)</td> <td>65</td> <td>213</td> <td>152</td> <td>528</td> </tr> <tr> <td>\(S_\text{GH} = \text{true}\)</td> <td>41</td> <td>88</td> <td>106</td> <td>174</td> </tr> </tbody> </table> <p><strong>Table 4</strong> - Performance with gradients and Hessian</p> <p>Table 4 shows the number of iterations and function evaluations by different solvers, given different values of \(n\). In both cases, the number of iterations is reduced dramatically after including analytical Hessian, but increases a bit if only including gradients. Nevertheless, including gradients avoids a substantial amount of function evaluations. Moreover, when \(n\) is larger, the efficiency introduced by adding gradients and Hessian becomes more noticeable.</p> <p>Meanwhile, we observe that, for default solver, \(N_\text{I}\) and \(N_\text{F}\) increases much faster when \(n\) grows. We expect that the computational complexity will soon become unaffordable if \(n\) becomes \(500, 1000\) or more. We attempt to reduce computing complexity by adapting the initial points.</p> <h3 id="modifying-initial-points">Modifying initial points</h3> <p>Instead of giving trivial initial guess, we make a wiser choice for at least \(\mathbf{x}^0\). A physically meaningful choice would be that \(x_i\)’s are evenly distributed within the interval \([x_0, x_{n+1}]\). Following this choice, we modify Table 4 to Table 5.</p> <table> <thead> <tr> <th> </th> <th>\(n=19\)</th> <th> </th> <th>\(n=49\)</th> <th> </th> </tr> </thead> <tbody> <tr> <td><strong>Solver</strong></td> <td>\(N_\text{I}\)</td> <td>\(N_\text{F}\)</td> <td>\(N_\text{I}\)</td> <td>\(N_\text{F}\)</td> </tr> <tr> <td><strong>Default</strong></td> <td>52</td> <td>2331</td> <td>140</td> <td>14782</td> </tr> <tr> <td>\(S_\text{G} = \text{true}\)</td> <td>52</td> <td>120</td> <td>152</td> <td>559</td> </tr> <tr> <td>\(S_\text{GH} = \text{true}\)</td> <td>10</td> <td>16</td> <td>13</td> <td>18</td> </tr> </tbody> </table> <p><strong>Table 5</strong> - Performance with wise initial points</p> <p>The changes due to initial points are significant, especially in the case of larger \(n\). In particular, the solver with gradients and Hessian gets most improvement.</p> <p>So far the observed most efficient numerical scheme is the gradient and Hessian solver with evenly distributed initial points \(\mathbf{x}^0\). We will further test the robustness of this scheme by scaling up the catenary problem, i.e. to allow \(n\) to grow to a very large number.</p> <h3 id="increasing-the-number-of-beams">Increasing the number of beams</h3> <p>Let \(Y=0\) rather then 5 such that the initial guess probably makes more sense. We run the chosen solver for \(n=99,\cdots,9999\) and show the number of iterations and function evaluations in Table 6. The chosen numerical scheme proves to be super efficient since the number of iterations seems follow a logarithmic growth with \(n\).</p> <table> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>\(n\)</td> <td>9</td> <td>49</td> <td>99</td> <td>499</td> <td>999</td> <td>4999</td> <td>9999</td> </tr> <tr> <td>\(N_\text{I}\)</td> <td>8</td> <td>11</td> <td>12</td> <td>14</td> <td>15</td> <td>18</td> <td>19</td> </tr> <tr> <td>\(N_\text{F}\)</td> <td>13</td> <td>16</td> <td>17</td> <td>19</td> <td>20</td> <td>23</td> <td>24</td> </tr> </tbody> </table> <p><strong>Table 6</strong> - Performance under different \(n\).</p> <h3 id="experiment-on-model-parameter-y">Experiment on model parameter Y</h3> <p>Suppose we set \(Y\) back to the default value 5 and re-run the same numerical scheme, will we get the same performance? The answer is no. In fact, \(n=99\) leads to \(N_\text{I} = 33\), \(n=499\) leads to \(N_\text{I} = 501\), and \(n=999\) leads to \(N_\text{I} = 1445\). This evidence implies the key role that the initial points play in the scheme. Our initial guess might not fit the case where \(Y=5\) well because more beams tend to group near the right side, far more from the assumption of even distribution. It is expected that our numerical scheme is less efficient given larger \(\\|Y\\|\) (either larger positive \(Y\) or smaller negative \(Y\)).</p> <h3 id="experiment-on-model-parameter-gamma">Experiment on model parameter gamma</h3> <p>Will the numerical optimisation performance be invariant with the model parameter \(\gamma\)? The answer is no, as well. Similar to \(Y\), the value of \(\gamma\) has a saying on the quality of our initial guess. The smaller the \(\gamma\) is, in physical sense more beams will stay closer to the two sides, so that the initial guess of even horizontal distribution is more unrealistic. This is evidenced by the observations shown in Table 7, where we run the numerical scheme given different \(\gamma\) (we fix \(n=499\)).</p> <table> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>\(\gamma\)</td> <td>0.01</td> <td>0.1</td> <td>0.2</td> <td>0.5</td> <td>0.7</td> <td>0.8</td> <td>0.9</td> <td>0.99</td> </tr> <tr> <td>\(N_\text{I}\)</td> <td>\(&gt;3000\)</td> <td>270</td> <td>242</td> <td>26</td> <td>14</td> <td>14</td> <td>15</td> <td>17</td> </tr> <tr> <td>\(N_\text{F}\)</td> <td>\(&gt;5000\)</td> <td>807</td> <td>578</td> <td>55</td> <td>19</td> <td>19</td> <td>20</td> <td>22</td> </tr> </tbody> </table> <p><strong>Table 7</strong> - Performance under different \(\gamma\)</p> <h3 id="termination-tolerance-and-realised-error">Termination tolerance and realised error</h3> <p>Finally we look at the comparison between termination tolerance (\(T_\text{1st}\) and \(T_\text{C}\)) and realised error (\(E_\text{1st}\) and \(E_\text{C}\)). We study the case where \(n=499\), \(Y=0\) and all the other parameters reserve default values.</p> <p>Recall that we assume very small tolerant step size and very large tolerant number of iterations and function evaluations. In implementation, we input <code class="language-plaintext highlighter-rouge">1e-100</code> for the stepsize, and <code class="language-plaintext highlighter-rouge">1e10</code> for the number of iterations and function evaluations. Now we lift \(T_\text{1st}\) and \(T_\text{C}\) up to <code class="language-plaintext highlighter-rouge">1e-100</code>, and obtain results shown in Figure 3. The optimisation stopped because the relative changes in all elements of \(\mathbf{x}\) are less than \texttt{1e-100}. Hence, it seems there’s a limit on achievable levels for \(E_\text{1st}\) and \(E_\text{C}\). Meanwhile, Figure 3 provides a good guidance to know how many iterations are required for the desired tolerance \(T_\text{1st}\) and \(T_\text{C}\).</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/hanging-catenary/HessianOptimPerform_Fst.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/hanging-catenary/HessianOptimPerform_C.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Figure 3</strong> - Realised errors over iterations.</p> <h2 id="conclusion">Conclusion</h2> <p>We propose a numerical minimisation scheme to solve the catenary problem. We observe that including gradients and Hessian can largely improve the efficiency of the scheme, and such improvement is more significant in large-scaled problem. Moreover, initial guess also plays a key role in improving efficiency, though it is difficult to come up with a perfect one.</p>]]></content><author><name>Victor Wang</name></author><category term="maths"/><category term="maths,"/><category term="physics"/><summary type="html"><![CDATA[A catenary is the curve that an idealised hanging chain or cable assumes under its own weight when supported only at its ends. For example, a stressed ribbon bridge hanging between pillars on either side of a river forms a shape of catenary. The catenary shape can be derived by minimising the total energy of the hanging chain. We would like to formulate a discrete energy objective and minimise it numerically.]]></summary></entry><entry><title type="html">Edge waves</title><link href="https://vicaws.github.io/blog/2017/edge-waves/" rel="alternate" type="text/html" title="Edge waves"/><published>2017-11-04T12:00:00+00:00</published><updated>2017-11-04T12:00:00+00:00</updated><id>https://vicaws.github.io/blog/2017/edge-waves</id><content type="html" xml:base="https://vicaws.github.io/blog/2017/edge-waves/"><![CDATA[<p>The waves on the sea surface at the beach can be modelled as the Stokes waves propagating on a sloping bottom. In the following sections, we have written down the governing equations for the surface waves, and then derive the dispersion relations for these waves.</p> <h2 id="stokes-waves-model">Stokes waves model</h2> <p>The waves on the sea are typical stoke waves, the waves at the interface between a inviscid, irrotational incompressible fluid and a gas, for which the modelling assumes constant pressure and no shear stress on the interface, and that the only external force on the system is gravity acting on the fluid.</p> <p>Take the beach to be at \(x=0\) and the sea to extend in \(0&lt;x \rightarrow \infty\) with \(-\infty &lt; y &lt; \infty\). The undisturbed surface of the sea is \(z = 0\) and the sea bed slopes away at an angle \(\beta\). Consider an inviscid, irrotational incompressible fluid occupying the region \(-x \tan \beta &lt; z &lt; h(x,y,t)\), where the bottom surface (\(z = -x \tan \beta\)) is rigid and impermeable, and the top surface (\(z = h(x,y,t)\)) is a free boundary. We describe the properties of the fluid by equation (\ref{eq:Irrotationality}) and (\ref{eq:Incompressibility}).</p> <p>\begin{align} \text{Irrotationality} \quad \exists \phi \text{ s.t. } \mathbf{u} = \nabla \phi, \label{eq:Irrotationality} \end{align} \begin{align} \text{Incompressibility} \quad \nabla \cdot \mathbf{u} = 0, \label{eq:Incompressibility} \end{align} where the fluid velocity \(\mathbf{u}\) can be expressed as the gradient of a scalar function velocity potential \(\phi(x,y,z,t)\).</p> <p>Now consider boundary conditions. The sea bed is impermeable, so that</p> <p>\begin{equation} \mathbf{u} \cdot \mathbf{n} = 0, \quad \text{on } z = -x \tan \beta \end{equation} where the unit normal vector is \(\mathbf{n} = (\sin \beta, 0, \cos \beta)\). On the free surface, we have the kinetic condition and the dynamic condition: on \(z = h(x,y,t)\),</p> \[\text{Kinetic:} \quad \Big( \frac{\partial}{\partial t} + \mathbf{u} \cdot \nabla \Big) (z-h) = 0,\] <p>\begin{equation} \text{Dynamic:} \quad \frac{\partial \phi}{\partial t} + \frac{1}{2} |\nabla \phi|^2 = -gh, \label{eq:SurfBond} \end{equation}</p> <h2 id="linearisation">Linearisation</h2> <p>We replace \(\mathbf{u}\) by \(\nabla \phi\) in all equations, and linearise them by considering small perturbations to a flat free surface and a static fluid:</p> <p>\begin{equation} \phi = \delta \bar{\phi}, \quad h = \delta \bar{h}. \end{equation}</p> <p>Keeping only leading order terms in \(\delta\), and dropping the overbar notations, we have</p> <p>\begin{equation} \nabla^2 \phi = 0, \quad \text{in } -x \tan \beta &lt; z &lt; 0, \label{eq:LinearGeneral} \end{equation} \begin{equation} \tan \beta \frac{\partial \phi}{\partial x} + \frac{\partial \phi}{\partial z} = 0, \quad \text{on } z = -x \tan \beta, \label{eq:LinearBottom} \end{equation} \begin{equation} -\frac{\partial h}{\partial t} + \frac{\partial \phi}{\partial z} = 0, \quad \text{on } z = 0, \label{eq:LinearSurfKinetic} \end{equation} \begin{equation} \frac{\partial \phi}{\partial t} + gh = 0, \quad \text{on } z = 0. \label{eq:LinearSurfDynamic} \end{equation}</p> <h2 id="solution-and-discussion">Solution and discussion</h2> <p>Too find solution from equations (\ref{eq:LinearGeneral})–(\ref{eq:LinearSurfDynamic}), we assume the waves have form \(\phi(x,y,z,t) = f(y-ct) F(x,z)\). Then equation (\ref{eq:LinearGeneral}) gives</p> <p>\begin{equation} \label{eq:SepVariables} \frac{\partial_{xx} F + \partial_{zz} F}{F} = - \frac{\partial_{yy} f}{f} = \lambda^2, \end{equation}</p> <p>where \(\lambda\) is an arbitrary constant. The equation for \(\phi\) shows wave phenomena, so that we attempt for a wave solution</p> <p>\begin{equation} f(y-ct) = C_1 e^{i\lambda (y-ct)}, \end{equation}</p> <p>with a constant \(C_1\). Also we seek for a decayed wave solution for the height of the sea with the following form</p> <p>\begin{equation} h(x,y,t) = C_2 e^{i \lambda (y-ct)} e^{-ax}, \end{equation}</p> <p>in which the last term means the waves decay when \(x \rightarrow \infty\) with a position constant \(a\).</p> <p>We further assume, by separation of variables, \(F(x,z) = G(x)H(z)\). Then equation (\ref{eq:LinearSurfDynamic}) gives</p> <p>\begin{equation} G(x) \cdot H(z) \cdot f \cdot (-i \lambda c) = -g C_3 \cdot f e^{-ax}. \end{equation}</p> <p>Picking \(x\)-related terms on both sides of the equation, we find</p> <p>\begin{equation} G(x) = C_4 e^{-ax}, \end{equation} for some constant \(C_4\).</p> <p>Now equation (\ref{eq:SepVariables}) can be written into</p> <p>\begin{equation} \frac{H’’}{H} = \lambda^2 - a^2. \end{equation}</p> <p>We can verify that \(H\) is not complex by checking the boundary condition on the sea surface. In fact, equation (\ref{eq:LinearSurfKinetic}) and (\ref{eq:LinearSurfDynamic}) give</p> <p>\begin{equation} \left. \frac{H’}{H} \right|_{z=0}= \frac{\lambda^2 c^2}{g} \label{eq:FinalSurf} \end{equation}</p> <p>Hence the solution of \(H\) is in the form of</p> <p>\begin{equation} H(z) = C_5 e^{kz} + C_6 e^{-kz}, \end{equation}</p> <p>where \(k = \sqrt{\lambda^2 - a^2}\), and \(C_5\), \(C_6\) are constants. Using the boundary condition on the sea bed, namely equation (\ref{eq:LinearBottom}), we have</p> <p>\begin{equation} \label{eq:FinalSeaBed} -a \tan \beta + k \frac{C_5 e^{kz} - C_6 e^{-kz}}{C_5 e^{kz} + C_6 e^{-kz}} = 0, \text{ at } z = -x \tan \beta. \end{equation} Equation (\ref{eq:FinalSeaBed}) holds true only when \(k = a \tan \beta\) and \(C_6 = 0\). Along with the definition of \(k\), we obtain</p> <p>\begin{equation} k = \lambda \sin \beta, \end{equation}</p> <p>and therefore</p> <p>\begin{equation} \label{eq:HDerivative} \frac{H’}{H} = \lambda \sin \beta. \end{equation}</p> <p>Note that the wave frequency \(\omega\) is denoted as \(\lambda c\) in all the abovementioned equations. Equation (\ref{eq:FinalSurf}) and (\ref{eq:HDerivative}) lead to the dispersion relation between wave frequency \(\omega\) and wave number \(\lambda\).</p> <p>\begin{equation} \label{eq:DispersionRelation} \omega^2 = g \lambda \sin \beta. \end{equation}</p> <p>Recall that the dispersion relation for the classical Stokes waves is given by</p> <p>\begin{equation} \omega^2 = g \lambda \tanh (\lambda H), \end{equation}</p> <p>where \(H\) is the constant mean depth of the sea. Consider a sea of infinite depth, i.e. \(H \rightarrow \infty\), the classical governing equation becomes</p> <p>\begin{equation} \omega^2 = g \lambda, \end{equation}</p> <p>which agrees with the relation (\ref{eq:DispersionRelation}) when \(\beta = \frac{\pi}{2}\).</p>]]></content><author><name>Victor Wang</name></author><category term="maths"/><category term="maths,"/><category term="physics"/><summary type="html"><![CDATA[The waves on the sea surface at the beach can be modelled as the Stokes waves propagating on a sloping bottom. In the following sections, we have written down the governing equations for the surface waves, and then derive the dispersion relations for these waves.]]></summary></entry></feed>