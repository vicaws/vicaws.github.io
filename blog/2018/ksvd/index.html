<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Sparse representation of signals with the K-SVD algorithm | Victor Wang (汪胜)</title> <meta name="author" content="Victor Wang (汪胜)"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://vicaws.github.io/blog/2018/ksvd/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Sparse representation of signals with the K-SVD algorithm",
      "description": "",
      "published": "February 26, 2018",
      "authors": [
        {
          "author": "Victor Wang",
          "authorURL": "",
          "affiliations": [
            {
              "name": "InFoMM, Oxford",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Victor </span>Wang (汪胜)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Portfolio</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/profile/">Profile</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Menu</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/repositories/">Repositories</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Sparse representation of signals with the K-SVD algorithm</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#sparse-representations-of-signals">Sparse representations of signals</a></div> <div><a href="#the-k-svd-algorithm">The K-SVD algorithm</a></div> <div><a href="#synthetic-experiments">Synthetic experiments</a></div> <div><a href="#appendix">Appendix</a></div> </nav> </d-contents> <p>We provide an executive summary of the paper by Michal Aharon, Michael Elad and Alfred Bruckstein <d-cite key="aharon"></d-cite> that proposes the novel K-SVD algorithm for designing overcomplete dictionaries for signals sparse representation. In the next sections, we firstly describe the mathematics of signals sparse representation, and then explain how the K-SVD algorithm works in solving the problem. Finally, we present some synthetic experiments to test our implementation of the K-SVD algorithm.</p> <h2 id="sparse-representations-of-signals">Sparse representations of signals</h2> <p>Suppose a signal is denoted by a numeric vector \(\mathbf{y} \in \mathbb{R}^n\), the sparse representation of the signal is \(\mathbf{y} \approx \mathbf{D} \mathbf{x}\), where \(\mathbf{D} \in \mathbb{R}^{n \times K}\) is called the overcomplete dictionary matrix assuming \(n &lt; K\) and \(\mathbf{x} \in \mathbb{R}^K\) is a sparse vector. In other words, the goal is to represent \(\mathbf{y}\) as a sparse linear combination of the \(K\) prototype signal atoms \(\{\mathbf{d}_j\}^K_{j=1}\) that form columns of the dictionary \(\mathbf{D}\).</p> <p>The application of sparse representation is to train a proper dictionary \(\mathbf{D}\) based on a set of signals \(\mathbf{Y} = \{\mathbf{y}_j\}_{j=1}^N\), so that all newly coming signals can be possibly stored at minimum space as a sparse vector by taking advantage of this dictionary. This can be formulated as,</p> \[\begin{equation} \min_{\mathbf{D}, \mathbf{X}} \left\{ ||\mathbf{Y} - \mathbf{DX}||_F^2 \right\}, \text{ s.t. } \forall i, ||\mathbf{x}_i||_0 \leq T_0, \label{eq:formula} \end{equation}\] <p>where \(\mathbf{X} = \{\mathbf{x}_i\}_{i=1}^{N} \in \mathbb{R}^{K \times N}\), and \(T_0\) defines the sparsity requirement. The objective targets to minimise the Frobenius norm by optimising the choice of dictionary \(\mathbf{D}\) and sparse matrix \(\mathbf{X}\).</p> <p>The algorithms of solving the sparse representation problem (\ref{eq:formula}) typically involves iteratively solving \(\mathbf{X}\) and \(\mathbf{D}\) in sequence, given the other variable as a known parameter. In fact, literatures denote the corresponding two subproblems,</p> <ul> <li> <p><strong>Sparse coding</strong> is to compute the representation coefficients \(\mathbf{x}\) based on the given signal \(\mathbf{y}\) and the dictionary \(\mathbf{D}\) by solving \eqref{eq:formula}.</p> </li> <li> <p><strong>Dictionary update</strong> is to search for better dictionary \(\mathbf{D}\) based on the given signals \(\mathbf{Y}\) and the solved coefficients matrix \(\mathbf{X}\) in the previous sparse coding stage. By “better” we mean to further minimise the objective in \eqref{eq:formula}.</p> </li> </ul> <h2 id="the-k-svd-algorithm">The K-SVD algorithm</h2> <p>The K-SVD algorithm introduced by Michal Aharon, Michael Elad and Alfred Bruckstein <d-cite key="aharon"></d-cite> focuses on solving the dictionary update problem as explained in the previous section. The K-SVD algorithm is a generalisation of the K-means clustering algorithm that updates dictionary columns along with a simultaneous update of the associated sparse representations, thus accelerating the convergence.</p> <p>To see this, we assume that both \(\mathbf{X}\) and \(\mathbf{D}\) are fixed, and we would like to update only one column of dictionary \(\mathbf{d}_k\), and the associated sparse representations \(\mathbf{x}^k_T\), namely the \(k\)-th row in \(\mathbf{X}\). Then the objective in (\ref{eq:formula}) is rewritten as,</p> \[\begin{align} ||\mathbf{Y} - \mathbf{D} \mathbf{X}||_F^2 &amp; = \left\Vert \left( \mathbf{Y} - \sum_{j \neq k} \mathbf{d}_j \mathbf{x}^k_T \right) - \mathbf{d}_k \mathbf{x}_T^k \right\Vert_F^2 \\ &amp; := \left\Vert \mathbf{E}_k - \mathbf{d}_k \mathbf{x}_T^k \right\Vert_F^2. \label{eq:errorUpdateD} \end{align}\] <p>The matrix \(\mathbf{E}_k\) represents the error for all the \(N\) samples when the \(k\)-th atom is removed. We can use SVD to find the closest rank-1 matrix that approximates \(\mathbf{E}_k\), which effectively minimizes (\ref{eq:errorUpdateD}). However, the resulting \(\mathbf{x}_T^k\) is unlikely to fulfil the sparsity requirements. The remedy is to require that the new \(\tilde{\mathbf{x}}_T^k\) to have the same support as the original \(\mathbf{x}_T^k\). To add the requirement, we revise the objective (\ref{eq:errorUpdateD}) to</p> <p>\begin{equation} \left\Vert \mathbf{E}_k \mathbf{\Omega}_k - \mathbf{d}_k \mathbf{x}_T^k \mathbf{\Omega}_k \right\Vert_F^2, \end{equation} where</p> \[\begin{equation} \mathbf{\Omega}_k = \begin{cases} 1 \text{, } i = \omega_k(j); \\ 0 \text{, otherwise;} \end{cases} \end{equation}\] <p>and that the support of \(\mathbf{x}_T^k\) is defined as \(\omega_k := \{i: 1 \leq i \leq K, \text{ } \mathbf{x}_T^k \neq 0 \}\). Multiplying \(\mathbf{\Omega}_k\) shrinks \(\mathbf{x}_T^k\) by discarding zero entries and also refines \(\mathbf{E}_k\) to a selection of error columns that correspond to samples that use the atom \(\mathbf{d}_k\).</p> <p>Thereafter, we can decompose \(\mathbf{E}_k \mathbf{\Omega}_k = \mathbf{U} \Delta \mathbf{V}^\top\), and update:</p> <ul> <li> <p>\(\mathbf{d}_k\) as the first column of \(\mathbf{U}\);</p> </li> <li> <p>the non-zero elements of \(\mathbf{x}_T^k\) as the first column of \(\mathbf{V}\) multiplied by the largest singular value, namely \(\Delta (1,1)\)</p> </li> </ul> <p>By far, the K-SVD algorithm completes updating the dictionary, which will be used in the next iteration for sparse coding, unless the convergence requirement is satisfied (or maximum iterations is reached).</p> <h2 id="synthetic-experiments">Synthetic experiments</h2> <p>We’ve implemented the K-SVD algorithm in Python (see the <a href="#appendix">Appendix</a>). Meanwhile, we solve the sparse coding problem by taking advantage of the <code class="language-plaintext highlighter-rouge">OrthogonalMatchingPursuit()</code> implementation maintained by <code class="language-plaintext highlighter-rouge">Scikit-learn</code>. Together we are able to iteratively solve the sparse representation problem.</p> <p>To test the algorithm and our implementation, we follow the same way as described in \cite{aharon} to simulate synthetic data (section V.A.) and measure performance (equation (25) in section V.D.). Moreover, we define a more general performance measure</p> \[\begin{equation} P(\epsilon) = \sum_{k=1}^K \mathbf{1}_{\{1-|\mathbf{d}_i^\top \tilde{\mathbf{d}}_i|&lt; \epsilon \} }, \label{eq:performance} \end{equation}\] <p>which counts the number of “successes” in recovering the dictionary. Here \(\mathbf{d}_i\) is the generating dictionary atom and \(\tilde{\mathbf{d}}_i\) is its corresponding element (closest column in \(\ell^2\) norm) in the recovered dictionary.</p> <p>We need to initialise a dictionary to feed into the sparse coding process. Therefore it is reasonable to investigate the sensitivity of the result against the dictionary initialisation. Nevertheless, we show the result from one single trial first. In Figure 1, we plot the relative error (\(\|\mathbf{Y}-\mathbf{DX}\|_F / \|\mathbf{Y}\|_F\)) over iterations on the left, and the histogram of column \(\ell^2\)-norm errors on the right. The errors tend to converge into a positive constant. More than that, dictionary update by K-SVD algorithm greatly reduces the error during each iteration. Unfortunately, the orthogonal matching pursuit algorithm in the next iteration always brings the error back to a high level. We are thinking that this could be caused by that the function implemented by <code class="language-plaintext highlighter-rouge">Skicit-learn</code> does not allow us to initialize \(\mathbf{X}\) from previous iteration, so that it always leads to another worse local minimum. Looking at the histogram, we observe that 95% of the recovered signals (columns) have less than 20% errors from the original signals.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/ksvd/SingleSim.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Figure 1</strong> - Error.</p> <p>Next we look at the performance measure as defined in (\ref{eq:performance}). In Figure 2, we show how the number of successes vary with tolerance. With \(\epsilon \geq 0.06\), we can obtain at least 45 successes out of 50 for dictionary recovery.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/ksvd/SingleSimSuccess.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Figure 2</strong> - Count of successes.</p> <p>As mentioned before, it is reasonable to investigate the sensitivity of the result against the dictionary initialization. We initialize the dictionary with i.i.d. uniformly distributed entries. Each column is then normalized to a unit \(\ell^2\)-norm. We show in Figure 3 how error evolves over iterations for 50 different initialisation. It turns out the errors are reasonably consistent by the end of iteration.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/ksvd/MultiSim.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Figure 3</strong> - Errors of multiple trials.</p> <h2 id="appendix">Appendix</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">scipy</span> <span class="k">as</span> <span class="n">sp</span>
<span class="kn">import</span> <span class="n">scipy.sparse</span> <span class="k">as</span> <span class="n">sps</span>

<span class="kn">from</span> <span class="n">scipy.linalg</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">svd</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">rankdata</span>

<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">OrthogonalMatchingPursuit</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">gen_rand_dict_mat</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">):</span>
  <span class="sh">'''</span><span class="s">
  Simulate dictionary D with unit-L2-norm column.

  Arguments:

      n_features: int
          Number of features for signals

      n_atoms: int
          Number of signal atoms in the dictionary D.

  Returns:

      D: 2-D matrix, shape=(n_features, n_atoms)
          Simulated dictionary
  </span><span class="sh">'''</span>

  <span class="n">D_raw</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">)</span>
  <span class="n">col_norms</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">D_raw</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">D</span> <span class="o">=</span> <span class="n">D_raw</span> <span class="o">/</span> <span class="n">col_norms</span>

  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mat</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gen_rand_measure</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_sparsity</span><span class="p">):</span>
  <span class="sh">'''</span><span class="s">
  Simulate sparse representation coefficient matrices X. The number of non-zero
  elements equals to n_sparsity, their values are uniformly distributed in
  interval [0.1], and their locations are also uniformly random.

  Arguments:

      n_samples: int
          Number of samples

      n_atoms: int
          Number of signal atoms in the dictionary D.

      n_sparsity: int
          Maximal number of non-zero elements allowed in each measurment vector.

  Returns:

      X: 2-D sparse matrix, shape=(n_atoms, n_samples)
          Each column contains the coefficients for sparse representation of signals.   
  </span><span class="sh">'''</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">([</span><span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">])</span>

  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
      <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">)</span>
      <span class="n">rank</span> <span class="o">=</span> <span class="nf">rankdata</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
      <span class="n">loc_indices</span> <span class="o">=</span> <span class="n">rank</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="n">n_sparsity</span><span class="p">]</span>

      <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">n_sparsity</span><span class="p">)</span>

      <span class="nf">for </span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">loc_indices</span><span class="p">,</span> <span class="n">vals</span><span class="p">):</span>
          <span class="n">X</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

  <span class="c1"># Compressed Sensing Column (CSC) matrix
</span>  <span class="k">return</span> <span class="n">sps</span><span class="p">.</span><span class="nf">csc_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calc_errors_pctile</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">Y_backup</span><span class="p">,</span> <span class="n">pct</span><span class="p">):</span>
  <span class="n">error_list</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_backup</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">error_list</span><span class="p">,</span> <span class="n">pct</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D_generating</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">k_codewords</span> <span class="o">=</span> <span class="n">D</span><span class="p">.</span><span class="n">shape</span>

  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k_codewords</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k_codewords</span><span class="p">):</span>
          <span class="k">if</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">D</span><span class="p">[:,</span><span class="n">j</span><span class="p">]).</span><span class="nf">transpose</span><span class="p">()</span><span class="o">*</span><span class="n">D_generating</span><span class="p">[:,</span><span class="n">k</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
              <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
              <span class="k">break</span>

  <span class="k">return</span> <span class="n">correct</span>

<span class="k">def</span> <span class="nf">calc_ksvd</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">,</span> <span class="n">n_sparsity</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">pct</span> <span class="o">=</span> <span class="mi">95</span><span class="p">):</span>
  <span class="sh">'''</span><span class="s">
  Arguments:

      Y: 2-D matrix, shape = (n_features, n_samples)
          Original signals, each column represents a signal.

      n_atoms: int
          Number of signal atoms in the dictionary D.

      n_sparsity: int
          Maximal number of non-zero elements allowed in each measurment vector.

      max_iter: int
          Maximal number of iterations allowed.

      tol: float
          Tolerance of convergence measured as L-2 norm ||y-Dx||_2 for one signal.

      pct: float
          Percentile of norm errors over signals. Convergence is reached if the
          pct-th percentile of signal norm error is less than tol.

  Returns:

      D: 2-D matrix, shape = (n_features, n_atoms)
          Dictionary of signal atoms.

      X: 2-D sparse matrix, shape = (n_atoms, n_samples)
          Each column contains the coefficients for sparse representation of
          signals.

      err_sparse_coding: dictionary(key:int, value:float)
          Dictionary with keys being iteration count, values being norm error
          after sparse coding.

      err_dict_update: dictionary(key:int, value:float)
          Dictionary with keys being iteration count, values being norm error
          after dictionary update.
  </span><span class="sh">'''</span>

  <span class="n">n_features</span><span class="p">,</span> <span class="n">n_samples</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="n">shape</span>

  <span class="c1"># Initialize dictionary
</span>  <span class="n">D_0</span> <span class="o">=</span> <span class="nf">gen_rand_dict_mat</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_atoms</span><span class="p">)</span>
  <span class="n">iter_count</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">err_sparse_coding</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">err_dict_update</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="n">D</span> <span class="o">=</span> <span class="n">D_0</span>
  <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
      <span class="n">iter_count</span> <span class="o">=</span> <span class="n">iter_count</span> <span class="o">+</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="n">iter_count</span> <span class="o">&gt;</span> <span class="n">max_iter</span><span class="p">:</span>
          <span class="k">break</span>

      <span class="c1"># Sparse coding
</span>      <span class="n">omp</span> <span class="o">=</span> <span class="nc">OrthogonalMatchingPursuit</span><span class="p">(</span><span class="n">n_nonzero_coefs</span><span class="o">=</span><span class="n">n_sparsity</span><span class="p">)</span>
      <span class="n">omp</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
      <span class="n">X</span> <span class="o">=</span> <span class="n">sps</span><span class="p">.</span><span class="nf">csc_matrix</span><span class="p">(</span><span class="n">omp</span><span class="p">.</span><span class="n">coef_</span><span class="p">.</span><span class="nf">transpose</span><span class="p">())</span>
      <span class="n">err_sparse_coding</span><span class="p">[</span><span class="n">iter_count</span><span class="p">]</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="o">-</span><span class="n">D</span><span class="o">*</span><span class="n">X</span><span class="p">)</span>
      <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Iteration = {}/{}, Sparse Codeing - Columm-Pct Norm error = {}</span><span class="sh">"</span><span class="p">.</span>
            <span class="nf">format</span><span class="p">(</span><span class="n">iter_count</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="nf">calc_errors_pctile</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">D</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="n">pct</span><span class="p">)))</span>

      <span class="c1"># Codebook update
</span>      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_atoms</span><span class="p">):</span>
          <span class="n">xk</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span>
          <span class="n">DX_k</span> <span class="o">=</span> <span class="n">D</span><span class="o">*</span><span class="n">X</span> <span class="o">-</span> <span class="n">D</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">xk</span>
          <span class="n">E</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">DX_k</span>

          <span class="n">nonzero_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">nonzero</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>
          <span class="n">nonzero_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">nonzero_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
          <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">nonzero_size</span><span class="p">)</span>
          <span class="n">Omega</span> <span class="o">=</span> <span class="n">sps</span><span class="p">.</span><span class="nf">csc_matrix</span><span class="p">((</span><span class="n">ones</span><span class="p">,</span> <span class="p">(</span><span class="n">nonzero_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nf">range</span><span class="p">(</span><span class="n">nonzero_size</span><span class="p">))),</span>
                                 <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">nonzero_size</span><span class="p">))</span>

          <span class="n">E_Omega</span> <span class="o">=</span> <span class="n">E</span> <span class="o">*</span> <span class="n">Omega</span>
          <span class="n">xk_Omega</span> <span class="o">=</span> <span class="n">xk</span> <span class="o">*</span> <span class="n">Omega</span>

          <span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="nf">svd</span><span class="p">(</span><span class="n">E_Omega</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="n">dk</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
          <span class="n">xk_Omega</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">Vh</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
          <span class="n">xk</span> <span class="o">=</span> <span class="n">sps</span><span class="p">.</span><span class="nf">csc_matrix</span><span class="p">((</span><span class="n">xk_Omega</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">nonzero_size</span><span class="p">),</span>
                               <span class="n">nonzero_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span>

          <span class="c1"># Update X_J
</span>          <span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">xk</span>
          <span class="c1"># Update D_J
</span>          <span class="n">D</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mat</span><span class="p">(</span><span class="n">dk</span><span class="p">).</span><span class="n">T</span>

      <span class="n">err_dict_update</span><span class="p">[</span><span class="n">iter_count</span><span class="p">]</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="o">-</span><span class="n">D</span><span class="o">*</span><span class="n">X</span><span class="p">)</span>
      <span class="n">err</span> <span class="o">=</span> <span class="nf">calc_errors_pctile</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">D</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="n">pct</span><span class="p">)</span>
      <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration = {}/{}, Dict Update    - Columm-Pct Norm error = {}</span><span class="sh">'</span><span class="p">.</span>
            <span class="nf">format</span><span class="p">(</span><span class="n">iter_count</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">err</span><span class="p">))</span>

      <span class="c1"># Termination condition
</span>      <span class="k">if</span> <span class="n">err</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
          <span class="k">break</span>

  <span class="k">return</span> <span class="n">D</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">err_sparse_coding</span><span class="p">,</span> <span class="n">err_dict_update</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

  <span class="c1"># Simulate test data
</span>  <span class="c1"># Y.shape = (n, N)
</span>  <span class="c1"># D.shape = (n, K)
</span>  <span class="c1"># X.shape = (K, N)
</span>  <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
  <span class="n">n</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">n_sparsity</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span>
  <span class="n">D</span> <span class="o">=</span> <span class="nf">gen_rand_dict_mat</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
  <span class="n">X</span> <span class="o">=</span> <span class="nf">gen_rand_measure</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">n_sparsity</span><span class="p">)</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="n">D</span> <span class="o">*</span> <span class="n">X</span>

  <span class="c1"># Run K-SVD and store results
</span>  <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">20</span>
  <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-2</span>
  <span class="n">pct</span> <span class="o">=</span> <span class="mi">95</span>
  <span class="n">result_D</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">result_X</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">result_errsc</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># error after sparse coding
</span>  <span class="n">result_errdu</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># error after dictionary update
</span>  <span class="k">for</span> <span class="n">i_sim</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
      <span class="n">D_J</span><span class="p">,</span> <span class="n">X_J</span><span class="p">,</span> <span class="n">err_sparse_coding</span><span class="p">,</span> <span class="n">err_dict_update</span> <span class="o">=</span> <span class="nf">calc_ksvd</span><span class="p">(</span>
        <span class="n">Y</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">n_sparsity</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">pct</span><span class="o">=</span><span class="n">pct</span><span class="p">)</span>
      <span class="n">result_D</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">D_J</span><span class="p">)</span>
      <span class="n">result_X</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">X_J</span><span class="p">)</span>
      <span class="n">result_errsc</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">err_sparse_coding</span><span class="p">)</span>
      <span class="n">result_errdu</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">err_dict_update</span><span class="p">)</span>

  <span class="c1"># Display results for a particular simulation
</span>  <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># the first simulation
</span>  <span class="n">D_J</span> <span class="o">=</span> <span class="n">result_D</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="n">X_J</span> <span class="o">=</span> <span class="n">result_X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="n">err_sparse_coding</span> <span class="o">=</span> <span class="n">result_errsc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="n">err_dict_update</span> <span class="o">=</span> <span class="n">result_errdu</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

  <span class="n">d_accuracy_tol</span> <span class="o">=</span> <span class="mf">0.07</span>
  <span class="n">success</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D_J</span><span class="p">,</span> <span class="n">d_accuracy_tol</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">With tolerance = {}. The number of success = {} out of {}</span><span class="sh">'</span><span class="p">.</span>\
    <span class="nf">format</span><span class="p">(</span><span class="n">d_accuracy_tol</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>

  <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
  <span class="n">error_list</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">D_J</span> <span class="o">*</span> <span class="n">X_J</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">norm_list</span> <span class="o">=</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">rel_err_list</span> <span class="o">=</span> <span class="n">error_list</span> <span class="o">/</span> <span class="n">norm_list</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">rel_err_list</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Relative Column Norm Errors</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Frequency of Samples</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Histogram of Column Errors</span><span class="sh">'</span><span class="p">)</span>

  <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">err_sparse_coding</span><span class="p">.</span><span class="nf">values</span><span class="p">()</span> <span class="o">/</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Sparse coding</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">err_dict_update</span><span class="p">.</span><span class="nf">values</span><span class="p">()</span> <span class="o">/</span> <span class="nf">norm</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Dictionary update</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Iteration</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Relative Norm Error</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Errors over Iteration</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
  <span class="nf">main</span><span class="p">()</span></code></pre></figure> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-02-26-ksvd.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Victor Wang (汪胜). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>